{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_Advanced.csv', sep = ',', header=[0,1], index_col =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = [('Infomation', 'Date')])\n",
    "df.drop(\"Rk\", level=1, axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df[('Predictor','R')] = 0\n",
    "df[('Predictor','PTS')] = 0\n",
    "df[('Predictor','PTSA')]= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(df)):\n",
    "    print('row: ' + str(i))\n",
    "    r, points = (df.loc[i,[('Infomation', 'Result')]][0]).split()\n",
    "    PTS, PTSA = points.split('-')\n",
    "    df.loc[[i],[('Predictor', 'R')]], df.loc[[i],[('Predictor', 'PTS')]], df.loc[[i],[('Predictor', 'PTSA')]] = r, PTS, PTSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_Advanced_clean.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_Advanced_clean.csv', sep = ',', header=[0,1], index_col =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[('Predictor', 'R')] = (df[('Predictor', 'R')] == 'W').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = [('Infomation', 'Date')], inplace =True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Infomation</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Team</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Opponent</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Differnces</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Predictor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Home/Away</th>\n",
       "      <th>Opp</th>\n",
       "      <th>Result</th>\n",
       "      <th>ORtg</th>\n",
       "      <th>FTr</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>TS%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>...</th>\n",
       "      <th>TS%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>ORtg</th>\n",
       "      <th>FTr</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>TS%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>R</th>\n",
       "      <th>PTS</th>\n",
       "      <th>PTSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1983-10-28</td>\n",
       "      <td>CLE</td>\n",
       "      <td>0</td>\n",
       "      <td>NYK</td>\n",
       "      <td>L 106-113</td>\n",
       "      <td>116.8</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1983-10-28</td>\n",
       "      <td>WSB</td>\n",
       "      <td>1</td>\n",
       "      <td>PHI</td>\n",
       "      <td>L 114-117</td>\n",
       "      <td>110.1</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983-10-28</td>\n",
       "      <td>PHI</td>\n",
       "      <td>0</td>\n",
       "      <td>WSB</td>\n",
       "      <td>W 117-114</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.561</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1983-10-28</td>\n",
       "      <td>UTA</td>\n",
       "      <td>1</td>\n",
       "      <td>DEN</td>\n",
       "      <td>L 125-139</td>\n",
       "      <td>115.6</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1983-10-28</td>\n",
       "      <td>DEN</td>\n",
       "      <td>0</td>\n",
       "      <td>UTA</td>\n",
       "      <td>W 139-125</td>\n",
       "      <td>128.6</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.500</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Infomation                                  Team                       \\\n",
       "         Date   Tm Home/Away  Opp     Result   ORtg    FTr   3PAr    TS%   \n",
       "0  1983-10-28  CLE         0  NYK  L 106-113  116.8  0.513  0.038  0.554   \n",
       "1  1983-10-28  WSB         1  PHI  L 114-117  110.1  0.354  0.024  0.602   \n",
       "2  1983-10-28  PHI         0  WSB  W 117-114  113.0  0.512  0.000  0.582   \n",
       "3  1983-10-28  UTA         1  DEN  L 125-139  115.6  0.320  0.020  0.548   \n",
       "4  1983-10-28  DEN         0  UTA  W 139-125  128.6  0.382  0.000  0.583   \n",
       "\n",
       "         ...  Opponent        Differnces                              \\\n",
       "    eFG% ...       TS%   eFG%       ORtg    FTr   3PAr    TS%   eFG%   \n",
       "0  0.513 ...     0.617  0.590       -7.7  0.115 -0.038 -0.062 -0.077   \n",
       "1  0.561 ...     0.582  0.512       -2.9 -0.159  0.024  0.019  0.049   \n",
       "2  0.512 ...     0.602  0.561        2.9  0.159 -0.024 -0.019 -0.049   \n",
       "3  0.500 ...     0.583  0.539      -12.9 -0.062  0.020 -0.035 -0.039   \n",
       "4  0.539 ...     0.548  0.500       12.9  0.062 -0.020  0.035  0.039   \n",
       "\n",
       "  Predictor            \n",
       "          R  PTS PTSA  \n",
       "0         0  106  113  \n",
       "1         0  114  117  \n",
       "2         1  117  114  \n",
       "3         0  125  139  \n",
       "4         1  139  125  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLE = df.where(df.loc[:, ('Infomation', 'Tm')] == 'CLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLE.dropna(axis=0, inplace=True)\n",
    "df_CLE.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[('Infomation', 'Date')] = pd.to_datetime(df[('Infomation', 'Date')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df_CLE.loc[:, [('Infomation', 'Date')]]\n",
    "df_CLE = pd.concat([df_date, df_CLE.loc[:, ['Team', 'Opponent', 'Differnces', 'Predictor']]], axis=1, sort=False)\n",
    "df_CLE.columns = df_CLE.columns.droplevel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLE_ =pd.DataFrame()\n",
    "l = []\n",
    "for i in range(3, len(df_CLE)):\n",
    "    row = (\n",
    "      [df_CLE.iloc[i, 0]]\n",
    "    + df_CLE.iloc[i-3,1:-3].tolist()\n",
    "    + df_CLE.iloc[i-2,1:-3].tolist()\n",
    "    + df_CLE.iloc[i-1,1:-3].tolist()\n",
    "    + df_CLE.iloc[i,-3:].tolist())\n",
    "    l.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CLE_ = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns =  ['Date',\n",
    "           'ORtg1.1', 'FTr1.1', '3PAr1.1', 'TS%1.1', 'eFG%1.1',\n",
    "           'ORtg1.2', 'FTr1.2', '3PAr1.2', 'TS%1.2', 'eFG%1.2',\n",
    "           'ORtg1.3', 'FTr1.3', '3PAr1.3', 'TS%1.3', 'eFG%1.3',\n",
    "           'ORtg2.1', 'FTr2.1', '3PAr2.1', 'TS%2.1', 'eFG%2.1',\n",
    "           'ORtg2.2', 'FTr2.2', '3PAr2.2', 'TS%2.2', 'eFG%2.2',\n",
    "           'ORtg2.3', 'FTr2.3', '3PAr2.3', 'TS%2.3', 'eFG%2.3',           \n",
    "           'ORtg3.1', 'FTr3.1', '3PAr3.1', 'TS%3.1', 'eFG%3.1',\n",
    "           'ORtg3.2', 'FTr3.2', '3PAr3.2', 'TS%3.2', 'eFG%3.2',\n",
    "           'ORtg3.3', 'FTr3.3', '3PAr3.3', 'TS%3.3', 'eFG%3.3',\n",
    "            'R', 'PTS', 'PTSA']\n",
    "\n",
    "df_CLE_.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc_X = StandardScaler()\n",
    "#X_std = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_CLE_['R']\n",
    "y = y.values.reshape(1, -1).flatten()\n",
    "X = df_CLE_.drop(['Date', 'R', 'PTS', 'PTSA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    '''\n",
    "    print the accuracy score, classification report and confusion matrix of classifier\n",
    "    '''\n",
    "    if train:\n",
    "        '''\n",
    "        training performance\n",
    "        '''\n",
    "        print(\"Train Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "\n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "        \n",
    "    elif train==False:\n",
    "        '''\n",
    "        test performance\n",
    "        '''\n",
    "        print(\"Test Result:\\n\")        \n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.5843\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.54      0.56       894\n",
      "        1.0       0.59      0.63      0.61       956\n",
      "\n",
      "avg / total       0.58      0.58      0.58      1850\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[480 414]\n",
      " [355 601]]\n",
      "\n",
      "Average Accuracy: \t 0.5664\n",
      "Accuracy SD: \t\t 0.0237\n"
     ]
    }
   ],
   "source": [
    "print_score(clf_LR, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.5718\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.55      0.57       416\n",
      "        1.0       0.55      0.60      0.57       378\n",
      "\n",
      "avg / total       0.57      0.57      0.57       794\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[228 188]\n",
      " [152 226]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(clf_LR, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  15 out of  15 | elapsed:   34.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 0 minutes and 39.98 seconds.\n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X, y)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All results:\n",
      "{'mean_fit_time': array([4.81135162, 8.90316224, 8.6664203 , 6.40477467, 7.61180957]), 'std_fit_time': array([0.11045593, 0.01151809, 0.133682  , 0.14735791, 0.24954798]), 'mean_score_time': array([0.05456614, 0.08130169, 0.08453488, 0.04292051, 0.04882868]), 'std_score_time': array([0.05733059, 0.0013712 , 0.02498349, 0.00382561, 0.0014759 ]), 'param_subsample': masked_array(data=[1.0, 0.6, 0.8, 1.0, 0.8],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[5, 1, 5, 5, 1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[3, 5, 5, 5, 4],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[5, 1.5, 1, 5, 1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_colsample_bytree': masked_array(data=[1.0, 0.8, 0.8, 0.6, 1.0],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 1.5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 1, 'colsample_bytree': 1.0}], 'split0_test_score': array([0.577019  , 0.55762219, 0.55658345, 0.56432263, 0.57152187]), 'split1_test_score': array([0.60945671, 0.59355668, 0.60059137, 0.60502918, 0.59316587]), 'split2_test_score': array([0.59228035, 0.58709397, 0.59268328, 0.5849502 , 0.58937206]), 'mean_test_score': array([0.59291917, 0.57941848, 0.58327892, 0.5847672 , 0.58468305]), 'std_test_score': array([0.01325533, 0.01564504, 0.0191611 , 0.01662517, 0.00943952]), 'rank_test_score': array([1, 5, 4, 2, 3]), 'split0_train_score': array([0.80299885, 0.99997423, 0.99910836, 0.96012096, 0.99649013]), 'split1_train_score': array([0.7612064 , 0.99993042, 0.99934802, 0.96105513, 0.99748614]), 'split2_train_score': array([0.76704934, 0.99991001, 0.99928008, 0.94658679, 0.99703674]), 'mean_train_score': array([0.77708486, 0.99993822, 0.99924548, 0.95592096, 0.99700434]), 'std_train_score': array([1.84785665e-02, 2.67916456e-05, 1.00852154e-04, 6.61126401e-03,\n",
      "       4.07264294e-04])}\n",
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1.0, gamma=5, learning_rate=0.02, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=5, missing=None, n_estimators=600,\n",
      "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)\n",
      "\n",
      " Best normalized gini score for 3-fold search with 5 parameter combinations:\n",
      "0.18583834516092934\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 405 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=4)]: Done 504 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=4)]: Done 1144 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=4)]: Done 1215 out of 1215 | elapsed: 30.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All results:\n",
      "{'mean_fit_time': array([ 4.37275441,  4.41748063,  4.40473851,  4.63310393,  4.18888958,\n",
      "        3.93181443,  4.30186955,  4.14607779,  3.93765243,  5.86725092,\n",
      "        5.80542032,  5.39283037,  5.68557358,  5.65266577,  5.30942734,\n",
      "        5.47579797,  5.53091828,  5.23759516,  7.45659407,  7.46152083,\n",
      "        6.89303923,  7.07757497,  7.1264267 ,  6.75797097,  6.77386316,\n",
      "        6.93131097,  6.5953486 ,  4.43451095,  4.50114417,  4.09453932,\n",
      "        4.52234975,  4.28930918,  3.97078482,  4.24387701,  4.23342999,\n",
      "        3.95325589,  5.85306334,  5.80686784,  5.39596216,  5.69078374,\n",
      "        5.67431291,  5.33933091,  5.52808301,  5.58603549,  5.24654587,\n",
      "        7.45162559,  7.45016789,  6.93739875,  7.11477439,  7.14409272,\n",
      "        6.86349829,  6.78703014,  6.93999108,  6.62610515,  4.3689359 ,\n",
      "        4.40684676,  3.84553035,  4.10573355,  4.30845682,  3.99831867,\n",
      "        4.27217038,  4.18977785,  3.84205715,  5.70110949,  5.70394556,\n",
      "        5.16682307,  5.4564515 ,  5.5260915 ,  5.20451117,  5.37141856,\n",
      "        5.24411257,  5.00501283,  7.36189612,  7.27055494,  6.84543578,\n",
      "        7.08469073,  7.15510217,  6.79840795,  6.76687002,  6.93144234,\n",
      "        6.59182596,  4.36372741,  4.31887039,  3.95803531,  4.4753983 ,\n",
      "        4.33687917,  4.21571112,  4.38757141,  4.1764129 ,  3.93013446,\n",
      "        5.93987751,  5.96042418,  5.81265338,  5.50015736,  5.34454862,\n",
      "        5.05523237,  5.16483665,  5.15096617,  4.85338712,  6.79839094,\n",
      "        6.56330736,  6.08700911,  6.30914187,  6.68036064,  6.52998757,\n",
      "        6.39934937,  6.71037396,  6.77057457,  4.44371478,  4.23751259,\n",
      "        3.53917408,  3.43913555,  3.68088015,  3.29031213,  3.26399843,\n",
      "        3.33439875,  3.2742722 ,  5.93414513,  5.30006258,  5.13525009,\n",
      "        5.78268997,  5.57162253,  5.1803089 ,  5.41930668,  5.23571428,\n",
      "        4.49858713,  6.92252795,  8.1064647 ,  6.38133128,  5.09913278,\n",
      "        5.25116467,  5.39112433,  5.69640247,  5.38768291,  5.37221543,\n",
      "        4.15105327,  3.99372784,  3.75175293,  3.9782234 ,  3.85187976,\n",
      "        3.57255109,  3.93849174,  3.7061557 ,  3.40742238,  5.24899252,\n",
      "        5.6961805 ,  5.18323   ,  5.24201488,  5.28418191,  5.16331267,\n",
      "        5.24376011,  5.1067338 ,  5.61470191,  8.62742567,  7.44624917,\n",
      "        6.45521975,  6.59411502,  6.56439614,  6.28086082,  6.34315459,\n",
      "        7.37059339,  6.57605553,  4.13007871,  4.16837668,  3.92448839,\n",
      "        4.19818195,  4.11783981,  3.85268871,  4.02735988,  4.04136388,\n",
      "        3.8925039 ,  5.6908989 ,  5.61036253,  4.99623863,  5.12915436,\n",
      "        5.68032861,  6.53299475,  6.58946967,  6.32402364,  5.75887489,\n",
      "        6.90735714,  6.77920628,  6.74162054,  7.72292423, 10.69695759,\n",
      "        9.88055746,  7.2810932 ,  6.35187936,  6.92423002,  4.60496362,\n",
      "        4.05372683,  3.81614876,  4.48939848,  5.41453687,  4.93849341,\n",
      "        4.57420063,  4.10038845,  3.60003805,  5.21297065,  5.34714421,\n",
      "        4.94472853,  5.28574173,  5.16704424,  4.74721003,  4.92794156,\n",
      "        4.93474221,  4.73505259,  6.61605938,  6.85679754,  6.39180104,\n",
      "        6.29726473,  6.32457169,  6.1038897 ,  6.54832482,  6.50260202,\n",
      "        6.02549632,  4.24027642,  4.1322279 ,  3.98379858,  4.16940451,\n",
      "        3.94818648,  3.65089655,  3.96974548,  3.83590508,  3.57810577,\n",
      "        5.35098179,  5.36107882,  4.94302527,  5.20906377,  5.18332465,\n",
      "        4.87002921,  5.30473153,  5.14243833,  4.84751805,  6.88350948,\n",
      "        6.79764279,  6.3140777 ,  6.45739222,  6.47183013,  6.41098404,\n",
      "        6.23284467,  6.41092499,  6.34969672,  4.05654446,  3.88597759,\n",
      "        3.36355233,  3.79067047,  3.97412316,  3.60351547,  3.91869195,\n",
      "        3.79677161,  3.34478545,  5.13757881,  5.0882105 ,  4.51165231,\n",
      "        5.02162083,  5.05028296,  4.76718632,  4.89778121,  4.94043549,\n",
      "        4.55939198,  6.63174907,  6.509926  ,  5.7894876 ,  6.28679784,\n",
      "        6.31155356,  5.73578723,  6.01077779,  6.22956975,  5.76892694,\n",
      "        5.02803763,  6.13784528,  5.39051708,  5.04679314,  5.27579204,\n",
      "        5.50566395,  6.22061483,  4.71558563,  4.17210857,  6.47499728,\n",
      "        7.34762335,  6.36837594,  7.11576756,  6.92731452,  6.2984577 ,\n",
      "        6.45979071,  6.54748662,  6.18396465,  9.54754909,  9.54863874,\n",
      "        8.76375445,  8.49939473,  8.74130718,  8.82356596,  9.14804649,\n",
      "        8.95545348,  7.97435578,  5.03848759,  4.66686479,  4.23733219,\n",
      "        4.80695407,  5.0676473 ,  4.81116629,  5.09406471,  5.07397763,\n",
      "        4.81385159,  7.10813514,  7.01201113,  6.57288074,  6.93600909,\n",
      "        6.95594176,  7.06634847,  8.52736568,  8.31198398,  6.53302296,\n",
      "        9.01978993,  9.07434011,  8.43266765,  8.40158598,  8.96080756,\n",
      "        8.47439798,  8.72907631,  8.4367671 ,  7.764913  ,  5.18816082,\n",
      "        5.08563526,  4.40367556,  5.31989185,  4.83468787,  4.74891615,\n",
      "        5.82403366,  5.14594309,  4.49704814,  6.64291875,  6.92442067,\n",
      "        6.79928414,  6.58943772,  6.66744479,  6.283108  ,  7.18599804,\n",
      "        6.88064869,  5.8583986 ,  9.16503652, 10.76588813, 10.36446095,\n",
      "       10.7395912 ,  8.9234515 ,  7.65507134,  8.20189261,  9.53483319,\n",
      "        9.36880167,  5.4277788 ,  5.10006634,  4.69422523,  5.32879551,\n",
      "        5.17170684,  4.481179  ,  4.78582986,  5.1034224 ,  4.79279987,\n",
      "        7.91074276,  9.07403374,  6.72754161,  6.16010745,  6.14955886,\n",
      "        5.73300783,  6.02975265,  6.18687558,  5.98278912,  8.5555954 ,\n",
      "        8.38959098,  7.51563207,  7.99339422,  8.0797073 ,  7.57192739,\n",
      "        7.65717419,  7.96429737,  7.53916399,  5.32266521,  5.94343328,\n",
      "        4.626664  ,  5.05665342,  4.96389119,  4.3593235 ,  4.68318017,\n",
      "        4.58700832,  4.34947999,  6.9549222 ,  6.90073339,  6.10639461,\n",
      "        7.48327128,  7.77130906,  7.48924637,  8.19108335,  7.12708378,\n",
      "        5.95591958,  8.92128062,  8.96606104,  7.57922761,  8.25498184,\n",
      "        8.44399587,  7.63443844,  9.0139788 ,  8.68540192,  6.24730484]), 'std_fit_time': array([0.28227461, 0.02366368, 0.13897717, 0.1270029 , 0.0428389 ,\n",
      "       0.03618477, 0.0129974 , 0.03338996, 0.01170661, 0.01659132,\n",
      "       0.01203324, 0.02390346, 0.01439129, 0.01769478, 0.0187605 ,\n",
      "       0.02882099, 0.01876905, 0.0581707 , 0.00509123, 0.05599064,\n",
      "       0.03561979, 0.03055975, 0.02140148, 0.02481365, 0.03103236,\n",
      "       0.01540512, 0.05460513, 0.08583873, 0.04872109, 0.06721567,\n",
      "       0.07117234, 0.1019381 , 0.01580513, 0.02415635, 0.01440249,\n",
      "       0.02785846, 0.02657337, 0.03162626, 0.03494337, 0.01177024,\n",
      "       0.02357706, 0.01391034, 0.02078774, 0.01003207, 0.03639215,\n",
      "       0.03227336, 0.04650574, 0.05071401, 0.01988086, 0.00690312,\n",
      "       0.01417715, 0.01525745, 0.02845602, 0.04858568, 0.01268082,\n",
      "       0.01298505, 0.06888613, 0.1681743 , 0.06887408, 0.07529973,\n",
      "       0.01332266, 0.02054878, 0.07905125, 0.05331651, 0.09254808,\n",
      "       0.01927151, 0.057275  , 0.02447788, 0.07174414, 0.03004407,\n",
      "       0.08283162, 0.08077601, 0.0305456 , 0.09688216, 0.06948924,\n",
      "       0.02911907, 0.00622602, 0.05387111, 0.02843333, 0.02738327,\n",
      "       0.03103978, 0.01776157, 0.03102279, 0.01505337, 0.00703963,\n",
      "       0.09762433, 0.05349122, 0.02887946, 0.00638874, 0.01583111,\n",
      "       0.012835  , 0.09555441, 0.20093709, 0.0816619 , 0.01340373,\n",
      "       0.02794663, 0.02436469, 0.07152541, 0.02004615, 0.05348517,\n",
      "       0.07967122, 0.03344877, 0.01241199, 0.12834166, 0.11151784,\n",
      "       0.06864881, 0.12216188, 0.1352056 , 0.05588847, 0.10817512,\n",
      "       0.057058  , 0.04002487, 0.21729336, 0.31843769, 0.01395286,\n",
      "       0.05389053, 0.39276151, 0.05221145, 0.10815233, 0.1372661 ,\n",
      "       0.06079299, 0.01889052, 0.04333928, 0.07120851, 0.10199939,\n",
      "       0.03864309, 0.14092026, 0.34374319, 1.09473249, 0.01811523,\n",
      "       0.05038333, 0.252114  , 0.13357054, 0.06621365, 0.00674773,\n",
      "       0.20205235, 0.01314591, 0.02478738, 0.02934039, 0.09321323,\n",
      "       0.0477923 , 0.01853493, 0.06709078, 0.06130688, 0.08318622,\n",
      "       0.05037477, 0.19082998, 0.00409459, 0.01359859, 0.08828857,\n",
      "       0.11095769, 0.04360264, 0.26690407, 0.00782952, 0.47989572,\n",
      "       0.06447079, 0.02767043, 0.01682656, 0.03129523, 0.14695627,\n",
      "       0.09847184, 0.26961143, 0.02486199, 0.04849946, 0.1000003 ,\n",
      "       0.03601414, 0.04494181, 0.03517163, 0.00442846, 0.03590417,\n",
      "       0.07949397, 0.13159204, 0.09843391, 0.18304777, 0.09693198,\n",
      "       0.29675145, 0.10709571, 0.14713385, 0.16188851, 0.11887172,\n",
      "       0.19879114, 0.13068343, 0.19694157, 0.30259189, 0.46036088,\n",
      "       1.06135744, 0.8498396 , 0.02827923, 0.22839431, 0.29269944,\n",
      "       0.10345148, 0.02320042, 0.15974103, 0.49122564, 0.45076733,\n",
      "       0.00960727, 0.08402292, 0.1473087 , 0.07789676, 0.04167575,\n",
      "       0.04485116, 0.07434472, 0.1082103 , 0.0267175 , 0.00482915,\n",
      "       0.03840402, 0.03577598, 0.01601843, 0.12640898, 0.19393301,\n",
      "       0.05114788, 0.01476824, 0.10977429, 0.24539197, 0.18305098,\n",
      "       0.13781909, 0.1026868 , 0.03401898, 0.01550661, 0.08367474,\n",
      "       0.08233973, 0.01415584, 0.00430327, 0.060947  , 0.01997787,\n",
      "       0.01353366, 0.01146644, 0.03742269, 0.01465142, 0.02974417,\n",
      "       0.04545385, 0.00766381, 0.11269935, 0.03963624, 0.00803745,\n",
      "       0.04102673, 0.01492455, 0.01864444, 0.01387591, 0.06813122,\n",
      "       0.10004705, 0.11358497, 0.06090216, 0.05752531, 0.10743251,\n",
      "       0.0224128 , 0.02739498, 0.1080572 , 0.08914058, 0.01779095,\n",
      "       0.03654898, 0.02712081, 0.0083233 , 0.02766159, 0.069683  ,\n",
      "       0.0227061 , 0.07280519, 0.06605668, 0.01247994, 0.01559605,\n",
      "       0.02659241, 0.0332038 , 0.06752681, 0.02075428, 0.00832158,\n",
      "       0.04320322, 0.02900192, 0.08255218, 0.01906436, 0.01293225,\n",
      "       0.51049039, 0.36437938, 0.14605339, 0.17054358, 0.10986084,\n",
      "       0.3590492 , 0.02055041, 0.42205483, 0.01633057, 0.3756115 ,\n",
      "       0.03102232, 0.10944095, 0.19245888, 0.24882497, 0.03580185,\n",
      "       0.0267779 , 0.01672101, 0.02263849, 0.04159772, 0.25621955,\n",
      "       0.13663696, 0.00628509, 0.09359273, 0.24194439, 0.16487549,\n",
      "       0.18041282, 0.03120128, 0.17157659, 0.10815013, 0.01729523,\n",
      "       0.11738082, 0.08655674, 0.00863387, 0.03490019, 0.01162077,\n",
      "       0.01114777, 0.03538958, 0.0199975 , 0.02329288, 0.08649169,\n",
      "       0.11304732, 0.41366588, 0.58619487, 0.57319555, 0.01300138,\n",
      "       0.03972942, 0.02777948, 0.07179744, 0.06451274, 0.09210377,\n",
      "       0.10211241, 0.23594514, 0.18326024, 0.09586265, 0.1492772 ,\n",
      "       0.24885   , 0.03713006, 0.04119558, 0.33743364, 0.36922604,\n",
      "       0.05324913, 0.22576407, 0.06720478, 0.06701904, 0.30433666,\n",
      "       0.32239063, 0.12163744, 0.04665816, 0.19008077, 0.20911441,\n",
      "       0.38188787, 0.0565542 , 0.57392408, 0.54988428, 0.2397932 ,\n",
      "       0.04852788, 0.21113449, 0.10700251, 0.54954149, 0.16836248,\n",
      "       0.05597526, 0.39289204, 0.0669835 , 0.08070266, 0.21834395,\n",
      "       0.28440914, 0.03352086, 0.06994148, 0.05605942, 0.01075094,\n",
      "       0.37359307, 0.18590257, 0.68001532, 0.03657783, 0.03407767,\n",
      "       0.07041032, 0.0304982 , 0.02505771, 0.05286932, 0.00652778,\n",
      "       0.01577816, 0.09186814, 0.05168266, 0.02885104, 0.09326071,\n",
      "       0.06978386, 0.04543842, 0.02519863, 0.31695694, 0.12978136,\n",
      "       0.2163535 , 0.05564692, 0.05084637, 0.13657813, 0.02781418,\n",
      "       0.04431676, 0.07842847, 0.03903934, 0.05686917, 0.29178161,\n",
      "       0.35183358, 0.06405288, 0.48236607, 0.52236957, 0.18095054,\n",
      "       0.19557327, 0.04440547, 0.11346205, 0.31496906, 0.05789161,\n",
      "       0.03506866, 0.22970428, 0.11669402, 0.07848799, 0.43831885]), 'mean_score_time': array([0.04759955, 0.04544115, 0.04552881, 0.04828842, 0.03973071,\n",
      "       0.04770088, 0.04656887, 0.04642288, 0.04002166, 0.05552951,\n",
      "       0.06128629, 0.055233  , 0.05872019, 0.05774283, 0.06034184,\n",
      "       0.05291231, 0.05213571, 0.05767314, 0.0757037 , 0.07932814,\n",
      "       0.07370345, 0.07214808, 0.07387829, 0.07126999, 0.06892713,\n",
      "       0.06934166, 0.0721701 , 0.04609776, 0.04632227, 0.04564492,\n",
      "       0.04793429, 0.04517182, 0.04249612, 0.04632688, 0.04566479,\n",
      "       0.04659271, 0.05991658, 0.05424078, 0.06312434, 0.05827014,\n",
      "       0.05577636, 0.05907194, 0.05929176, 0.0603524 , 0.05273851,\n",
      "       0.07359632, 0.0720493 , 0.07251382, 0.07219172, 0.07474947,\n",
      "       0.0687321 , 0.07192508, 0.07299709, 0.06917183, 0.04800073,\n",
      "       0.03986716, 0.04052353, 0.04599579, 0.04590265, 0.04069765,\n",
      "       0.04486918, 0.04608234, 0.03764145, 0.04444917, 0.0454789 ,\n",
      "       0.05086295, 0.05278508, 0.05394777, 0.05149229, 0.05521949,\n",
      "       0.05341864, 0.0586435 , 0.07609685, 0.07192731, 0.06210351,\n",
      "       0.06774743, 0.07028437, 0.07276837, 0.07189139, 0.06553388,\n",
      "       0.06988867, 0.04695861, 0.04369171, 0.04289309, 0.04585417,\n",
      "       0.04404243, 0.04226049, 0.04838943, 0.04307874, 0.03750475,\n",
      "       0.04613082, 0.04584797, 0.03985985, 0.05045374, 0.04802378,\n",
      "       0.03419463, 0.05213722, 0.04718947, 0.0469083 , 0.05739379,\n",
      "       0.05709728, 0.05838664, 0.05746539, 0.0551029 , 0.05531828,\n",
      "       0.06144786, 0.05411545, 0.06465713, 0.03357832, 0.04143039,\n",
      "       0.0200518 , 0.03330032, 0.03316728, 0.02165516, 0.03582756,\n",
      "       0.03084381, 0.01629623, 0.04911868, 0.04352498, 0.03311245,\n",
      "       0.04748233, 0.04768936, 0.04062263, 0.04012974, 0.04626997,\n",
      "       0.03210473, 0.05913456, 0.03907617, 0.0298655 , 0.04578829,\n",
      "       0.04330341, 0.03328641, 0.03947886, 0.04668895, 0.03989061,\n",
      "       0.03300214, 0.03115575, 0.02956533, 0.03527387, 0.03235936,\n",
      "       0.02911401, 0.03366605, 0.03162781, 0.02912895, 0.03909532,\n",
      "       0.04244256, 0.04419859, 0.04205489, 0.03950342, 0.04291614,\n",
      "       0.04047147, 0.04088831, 0.03617509, 0.04836241, 0.05105495,\n",
      "       0.04724034, 0.05079786, 0.04693127, 0.04861474, 0.04573202,\n",
      "       0.04753002, 0.04634786, 0.0300746 , 0.03139758, 0.03170355,\n",
      "       0.03067923, 0.03237104, 0.03211109, 0.03178811, 0.03216608,\n",
      "       0.03394802, 0.04239058, 0.03914642, 0.03854521, 0.04268654,\n",
      "       0.04344161, 0.03841114, 0.03610659, 0.03795759, 0.0387694 ,\n",
      "       0.05139089, 0.05144548, 0.04795122, 0.0494074 , 0.05097159,\n",
      "       0.0467391 , 0.0483795 , 0.04841367, 0.04480338, 0.03050256,\n",
      "       0.02707569, 0.02826508, 0.03006339, 0.03050868, 0.03158498,\n",
      "       0.03159547, 0.03008604, 0.03036094, 0.0393246 , 0.04073207,\n",
      "       0.03757898, 0.0389475 , 0.03922105, 0.04034313, 0.03749943,\n",
      "       0.03879333, 0.03772783, 0.05162557, 0.05103882, 0.05105813,\n",
      "       0.05015635, 0.05295038, 0.04519653, 0.04084698, 0.04900233,\n",
      "       0.04607002, 0.02962518, 0.03031611, 0.02776893, 0.03249534,\n",
      "       0.03250957, 0.03115455, 0.0347929 , 0.03424843, 0.03404593,\n",
      "       0.03906139, 0.04166691, 0.03854537, 0.04205473, 0.03884164,\n",
      "       0.03957685, 0.04247761, 0.04019348, 0.03959394, 0.05078642,\n",
      "       0.05170727, 0.0490133 , 0.05317402, 0.05132254, 0.0497582 ,\n",
      "       0.04602893, 0.04953853, 0.04698594, 0.02825681, 0.02868573,\n",
      "       0.01620777, 0.03085788, 0.03013452, 0.01510644, 0.03196343,\n",
      "       0.03211824, 0.01487263, 0.03803293, 0.03321028, 0.02253683,\n",
      "       0.03657381, 0.03442216, 0.02224493, 0.03623907, 0.03388921,\n",
      "       0.02109273, 0.04700009, 0.04224173, 0.02453351, 0.04510498,\n",
      "       0.04374679, 0.02936236, 0.04281576, 0.04259555, 0.03242819,\n",
      "       0.03143899, 0.03458293, 0.0333782 , 0.03105442, 0.03372463,\n",
      "       0.03173224, 0.03203678, 0.03197877, 0.03021598, 0.03766537,\n",
      "       0.04154277, 0.037702  , 0.04261978, 0.04126263, 0.04417666,\n",
      "       0.04222417, 0.04309503, 0.03947139, 0.0540452 , 0.05755965,\n",
      "       0.05330038, 0.0567704 , 0.05535261, 0.05053576, 0.0526708 ,\n",
      "       0.05426327, 0.05143412, 0.03039098, 0.03219263, 0.02894886,\n",
      "       0.03208168, 0.03369594, 0.03124134, 0.03737458, 0.03636765,\n",
      "       0.03323348, 0.04114771, 0.04664437, 0.04052949, 0.04331795,\n",
      "       0.04313493, 0.04276156, 0.03830171, 0.04381927, 0.0427417 ,\n",
      "       0.06003539, 0.05809093, 0.05149976, 0.05179755, 0.06230537,\n",
      "       0.05550488, 0.05335196, 0.04820625, 0.04789567, 0.03144558,\n",
      "       0.03030292, 0.0302395 , 0.02902214, 0.03224564, 0.03433402,\n",
      "       0.03297989, 0.03019619, 0.0317781 , 0.04368607, 0.03919021,\n",
      "       0.0408717 , 0.04131365, 0.04528737, 0.04031237, 0.05714417,\n",
      "       0.03896666, 0.03612494, 0.06089155, 0.05302612, 0.05681888,\n",
      "       0.05246679, 0.05221446, 0.04547898, 0.05143897, 0.05310949,\n",
      "       0.05077211, 0.03193609, 0.03073128, 0.02740566, 0.02758177,\n",
      "       0.03032915, 0.02906386, 0.03573012, 0.03562307, 0.03479473,\n",
      "       0.04058599, 0.04136515, 0.03674833, 0.0397199 , 0.04161819,\n",
      "       0.03430382, 0.03773451, 0.04063932, 0.04133526, 0.06008156,\n",
      "       0.05265355, 0.04027907, 0.04830599, 0.05283761, 0.04584591,\n",
      "       0.05181845, 0.04823454, 0.04563618, 0.0316267 , 0.03582454,\n",
      "       0.01346286, 0.03172302, 0.0292511 , 0.01542147, 0.02995324,\n",
      "       0.0319802 , 0.01444236, 0.04079127, 0.03614179, 0.01392881,\n",
      "       0.04725297, 0.04116376, 0.0161562 , 0.04219047, 0.03508568,\n",
      "       0.01538817, 0.04498792, 0.04456433, 0.01537363, 0.04666448,\n",
      "       0.04417674, 0.01736903, 0.0440882 , 0.04505642, 0.01661372]), 'std_score_time': array([3.05236127e-03, 4.08339746e-03, 3.38589620e-03, 3.06355603e-03,\n",
      "       9.76052538e-03, 2.45977908e-03, 1.63151366e-03, 1.66642911e-03,\n",
      "       7.84816733e-03, 6.06275370e-03, 2.48178572e-03, 2.42951513e-03,\n",
      "       1.99013065e-03, 1.91688327e-03, 1.97415605e-03, 5.53465096e-03,\n",
      "       5.18940288e-03, 2.22724464e-03, 2.86924152e-03, 2.03817903e-03,\n",
      "       4.84482989e-03, 2.19174008e-03, 4.96727858e-03, 4.31958813e-03,\n",
      "       6.23378776e-03, 4.82298172e-03, 2.67995083e-03, 3.93313281e-03,\n",
      "       1.35530052e-04, 1.95547851e-03, 9.20608839e-04, 1.58939242e-03,\n",
      "       3.04044413e-03, 8.28932016e-04, 5.02651050e-03, 9.23981619e-04,\n",
      "       4.45678833e-03, 4.74936650e-03, 2.19779414e-03, 2.27539196e-03,\n",
      "       4.40340939e-03, 2.50708832e-03, 2.20586900e-03, 1.22057834e-03,\n",
      "       7.18306210e-03, 2.77836842e-03, 9.31357918e-03, 7.25001092e-03,\n",
      "       2.51526654e-03, 2.17065652e-03, 4.53644295e-03, 1.43491968e-03,\n",
      "       3.43108512e-03, 5.18494092e-03, 1.81624898e-03, 7.49903866e-03,\n",
      "       2.92392370e-04, 2.96151247e-03, 3.40404366e-04, 4.67306482e-03,\n",
      "       3.79437982e-03, 2.02566021e-03, 5.01101736e-03, 3.47381150e-03,\n",
      "       5.08290151e-03, 6.63308369e-03, 3.48623190e-03, 1.03997084e-02,\n",
      "       1.23573436e-02, 5.50628877e-03, 6.83510028e-03, 1.80887194e-03,\n",
      "       2.40989088e-03, 4.09258872e-03, 1.29563354e-02, 7.40004557e-03,\n",
      "       6.31234918e-03, 1.60480446e-03, 1.95939268e-03, 6.60590713e-03,\n",
      "       6.74903839e-03, 4.34948694e-03, 3.62195134e-03, 8.99160648e-04,\n",
      "       1.50825220e-03, 3.22101892e-03, 5.98130742e-03, 3.26647545e-03,\n",
      "       3.29068465e-03, 7.97666262e-03, 5.73323059e-03, 2.83619118e-03,\n",
      "       3.27605806e-03, 3.67801881e-03, 8.01187337e-03, 9.00182139e-04,\n",
      "       7.15280465e-03, 4.10130883e-03, 7.16269830e-03, 2.81858937e-03,\n",
      "       4.10940619e-03, 2.97810742e-03, 1.74695852e-03, 7.83856876e-03,\n",
      "       5.29272114e-03, 4.93409229e-03, 7.54218486e-03, 2.83275334e-02,\n",
      "       5.47138939e-03, 4.67182311e-03, 3.25718946e-03, 1.24323699e-03,\n",
      "       8.14076583e-04, 2.77295896e-03, 1.66542385e-03, 2.34300216e-03,\n",
      "       1.03615980e-03, 1.74179816e-02, 2.51632599e-03, 6.37559497e-03,\n",
      "       1.21536308e-02, 8.45661503e-03, 2.61355424e-03, 4.48971979e-03,\n",
      "       4.61854389e-03, 3.22782837e-03, 6.33811670e-03, 1.60867839e-03,\n",
      "       2.77733562e-03, 1.77331543e-03, 2.09737550e-03, 3.67790925e-03,\n",
      "       2.31042436e-03, 1.94886920e-03, 4.34979742e-03, 6.08860121e-04,\n",
      "       1.22276570e-03, 1.44998281e-03, 2.61558116e-03, 2.12028511e-03,\n",
      "       1.87276364e-03, 2.75622564e-03, 4.42933083e-03, 8.98419002e-04,\n",
      "       1.69108165e-03, 3.88702475e-03, 1.69103579e-03, 2.06610556e-03,\n",
      "       1.14307180e-03, 2.97176578e-04, 1.79521776e-03, 5.63781966e-03,\n",
      "       9.96786542e-04, 3.13059735e-03, 4.39169589e-03, 9.93957388e-04,\n",
      "       6.26940361e-04, 3.58864574e-03, 1.96320804e-03, 1.56472513e-03,\n",
      "       2.66284634e-03, 9.15339156e-04, 8.86308040e-04, 2.62249579e-03,\n",
      "       8.05642128e-04, 1.32200238e-03, 1.40686567e-03, 4.69735361e-04,\n",
      "       1.47960630e-03, 8.43552861e-04, 1.32493736e-03, 1.36737455e-03,\n",
      "       1.62526489e-03, 9.25747574e-04, 2.68807341e-03, 7.29019254e-03,\n",
      "       1.05975742e-03, 2.77473633e-03, 3.37891222e-03, 3.35318711e-03,\n",
      "       1.50445805e-03, 4.62827581e-03, 1.08107497e-03, 5.12408983e-03,\n",
      "       2.85298515e-03, 2.47812049e-03, 4.60692523e-03, 1.87995586e-03,\n",
      "       1.50108297e-03, 1.17596930e-03, 1.85100390e-03, 1.72473589e-03,\n",
      "       2.55175330e-03, 8.30423498e-04, 1.14094672e-03, 7.16914347e-04,\n",
      "       1.80064850e-03, 2.58011498e-03, 1.17327372e-03, 2.96271549e-03,\n",
      "       2.20702936e-03, 8.88924232e-04, 2.82815901e-03, 2.73968127e-03,\n",
      "       8.60140771e-04, 2.19013485e-03, 1.08223658e-03, 5.80723798e-03,\n",
      "       1.87415522e-03, 4.54708641e-03, 2.35392828e-03, 1.49516442e-03,\n",
      "       3.13741697e-03, 1.51697222e-03, 1.14312309e-03, 7.42046471e-04,\n",
      "       1.88407687e-03, 2.35051827e-03, 1.01045684e-03, 2.35893261e-03,\n",
      "       3.18586222e-03, 1.46927390e-03, 1.98933628e-03, 1.45168828e-03,\n",
      "       2.00951620e-03, 4.04961447e-03, 4.24676483e-03, 1.48639441e-03,\n",
      "       2.51327633e-03, 1.39561378e-03, 2.33267765e-03, 1.52542351e-03,\n",
      "       3.16606742e-03, 1.59654058e-03, 5.11526208e-04, 2.67640455e-03,\n",
      "       1.69651710e-03, 1.10506006e-03, 2.98965663e-03, 1.49501188e-03,\n",
      "       2.10002782e-03, 2.32106564e-03, 4.79395770e-04, 8.28375722e-04,\n",
      "       1.15237131e-03, 1.61250169e-03, 3.16910407e-03, 8.14947588e-04,\n",
      "       5.22939871e-05, 2.02821752e-03, 2.78026347e-03, 6.27863225e-04,\n",
      "       1.04202075e-03, 8.50857377e-04, 1.11591526e-03, 1.34778814e-03,\n",
      "       2.70739331e-03, 1.44546092e-03, 2.18271692e-03, 3.96994037e-04,\n",
      "       9.38535422e-04, 7.00693676e-04, 2.57809932e-04, 2.41324818e-03,\n",
      "       2.21631860e-03, 1.56518900e-03, 1.52556764e-03, 1.01857846e-03,\n",
      "       1.80532937e-03, 1.19090148e-03, 9.15679454e-04, 6.91931532e-03,\n",
      "       1.88859631e-03, 9.43555201e-04, 3.74077060e-03, 1.77005707e-03,\n",
      "       2.34720397e-04, 1.91152460e-03, 1.02072974e-03, 1.19656055e-03,\n",
      "       1.21125729e-03, 1.94667119e-03, 2.70076144e-03, 1.68801221e-03,\n",
      "       1.87646299e-03, 1.92187220e-03, 1.64104520e-03, 1.15000371e-03,\n",
      "       2.44867828e-03, 2.75802679e-04, 1.70051571e-03, 2.40670546e-03,\n",
      "       2.75781664e-03, 2.56577792e-03, 2.60182368e-03, 3.45592496e-03,\n",
      "       6.12878669e-04, 3.72314273e-03, 3.57133907e-03, 9.17644262e-04,\n",
      "       9.26076696e-04, 4.28194899e-03, 1.46289897e-03, 3.25849323e-03,\n",
      "       2.28950076e-03, 2.90781730e-03, 1.19027936e-03, 2.73277912e-03,\n",
      "       4.81038252e-03, 8.07591816e-03, 5.19288026e-03, 2.36522388e-03,\n",
      "       3.64527143e-03, 6.67616451e-03, 2.82240612e-03, 1.95081673e-03,\n",
      "       3.68037550e-03, 2.15718988e-03, 2.64933356e-03, 3.30143446e-03,\n",
      "       6.11934231e-03, 8.93191134e-03, 1.35850837e-03, 2.29469766e-03,\n",
      "       2.13996690e-03, 1.88086929e-03, 5.77804703e-04, 1.88132917e-03,\n",
      "       3.79368257e-03, 9.17867326e-03, 4.97966967e-03, 1.17090771e-03,\n",
      "       6.61031953e-04, 2.12335111e-03, 2.77079691e-03, 4.71570085e-03,\n",
      "       1.40860065e-03, 8.53865266e-03, 1.07291653e-03, 8.55656108e-03,\n",
      "       3.53761246e-03, 8.00006096e-04, 5.42106165e-03, 7.37711168e-04,\n",
      "       4.25913812e-03, 6.42464833e-03, 2.66663455e-03, 2.10338816e-03,\n",
      "       1.99937941e-03, 6.67580907e-03, 3.30856019e-03, 4.18254785e-03,\n",
      "       2.60345058e-03, 2.70436494e-03, 4.69651291e-04, 2.99173254e-03,\n",
      "       1.00535466e-03, 4.55108358e-03, 3.17437169e-03, 2.88633265e-03,\n",
      "       4.60440099e-03, 4.31083966e-03, 2.60762539e-03, 9.72274246e-04,\n",
      "       2.19024405e-03, 5.48142999e-03, 1.28747150e-03, 1.86347596e-03,\n",
      "       1.53851192e-03, 1.13242163e-02, 1.05671258e-03, 1.72159155e-03,\n",
      "       1.33081159e-03, 3.38165484e-03, 3.90792778e-03, 1.49422630e-04,\n",
      "       1.60018638e-03, 4.24564051e-03, 5.40129315e-04, 9.53332771e-03,\n",
      "       1.37570856e-03, 1.60091452e-03, 6.46430316e-04, 1.76117972e-03,\n",
      "       1.70151939e-03, 2.34385684e-03, 2.74608998e-03, 6.85620677e-03,\n",
      "       2.00956481e-03, 2.83450314e-03, 5.22419661e-03, 2.37253134e-03,\n",
      "       3.62668323e-03, 5.19239663e-03, 2.33254034e-03, 1.15919970e-03,\n",
      "       3.68719519e-03, 1.34360003e-03, 5.81312325e-04, 3.70142369e-03,\n",
      "       1.03902945e-03, 3.20962292e-03, 1.86280003e-03, 1.72495380e-03,\n",
      "       1.21201905e-03]), 'param_colsample_bytree': masked_array(data=[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6,\n",
      "                   0.6, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                   2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 2, 2, 2,\n",
      "                   2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                   2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "                   0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "                   1.5, 1.5, 1.5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "                   2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 5, 5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "                   5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10,\n",
      "                   10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10,\n",
      "                   10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5,\n",
      "                   10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5,\n",
      "                   5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1,\n",
      "                   1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10,\n",
      "                   1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10,\n",
      "                   10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10,\n",
      "                   10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5,\n",
      "                   10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5,\n",
      "                   5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1,\n",
      "                   1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10,\n",
      "                   1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10,\n",
      "                   10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10,\n",
      "                   10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5,\n",
      "                   10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5,\n",
      "                   5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1,\n",
      "                   1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10,\n",
      "                   1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10,\n",
      "                   10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10,\n",
      "                   10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5,\n",
      "                   10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5,\n",
      "                   5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10, 1, 1,\n",
      "                   1, 5, 5, 5, 10, 10, 10, 1, 1, 1, 5, 5, 5, 10, 10, 10,\n",
      "                   1, 1, 1, 5, 5, 5, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_subsample': masked_array(data=[0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.6, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.8, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 0.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 2, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0}], 'split0_test_score': array([0.56873473, 0.56585504, 0.57020544, 0.56532538, 0.56141208,\n",
      "       0.56884272, 0.56244568, 0.56209086, 0.55606922, 0.56154064,\n",
      "       0.55910318, 0.57152701, 0.55516417, 0.55360605, 0.56234284,\n",
      "       0.56380326, 0.56081557, 0.55693827, 0.55927802, 0.56263595,\n",
      "       0.57131618, 0.55806958, 0.55147713, 0.56337644, 0.55574011,\n",
      "       0.55030468, 0.55248502, 0.57320855, 0.56735659, 0.56851361,\n",
      "       0.56722289, 0.56377754, 0.56947008, 0.56150464, 0.56397295,\n",
      "       0.55810557, 0.55987967, 0.55905176, 0.5639421 , 0.55223819,\n",
      "       0.55174453, 0.56055846, 0.5636747 , 0.55886149, 0.55572468,\n",
      "       0.55791531, 0.5623017 , 0.56905356, 0.55433626, 0.55065436,\n",
      "       0.55282956, 0.55899005, 0.55255702, 0.55947857, 0.56906898,\n",
      "       0.56730517, 0.5692284 , 0.56801481, 0.56173605, 0.56481629,\n",
      "       0.56160749, 0.56026534, 0.55853238, 0.55929859, 0.56119096,\n",
      "       0.56824621, 0.55913918, 0.55174967, 0.55937058, 0.56415807,\n",
      "       0.55455223, 0.55823413, 0.55629034, 0.56117553, 0.5683902 ,\n",
      "       0.55539043, 0.55204278, 0.56217314, 0.55673257, 0.55202736,\n",
      "       0.5548762 , 0.57127504, 0.56906898, 0.57237035, 0.56835934,\n",
      "       0.56206001, 0.56725889, 0.56362842, 0.56075901, 0.55612578,\n",
      "       0.55676343, 0.55893348, 0.56934667, 0.55411514, 0.55204793,\n",
      "       0.56618415, 0.5629702 , 0.55310724, 0.55583267, 0.55602808,\n",
      "       0.55859409, 0.5671149 , 0.55242846, 0.5521302 , 0.55816214,\n",
      "       0.55918546, 0.55030468, 0.55453166, 0.57459697, 0.56623557,\n",
      "       0.57345024, 0.57293858, 0.56626642, 0.5752449 , 0.56783483,\n",
      "       0.5684879 , 0.5758157 , 0.56698635, 0.56064073, 0.56672923,\n",
      "       0.56081557, 0.5606613 , 0.57324454, 0.56308847, 0.56509912,\n",
      "       0.57006145, 0.56137094, 0.56467231, 0.57602653, 0.55488134,\n",
      "       0.55817242, 0.56432263, 0.56304733, 0.55599208, 0.56435862,\n",
      "       0.56370555, 0.56817422, 0.57150644, 0.56364899, 0.56504255,\n",
      "       0.56872959, 0.56746458, 0.55962255, 0.55626462, 0.56081043,\n",
      "       0.57007688, 0.57070938, 0.55521045, 0.55536472, 0.56275937,\n",
      "       0.55961227, 0.5539043 , 0.55491734, 0.55639832, 0.56209601,\n",
      "       0.56977348, 0.55427969, 0.56093642, 0.55593037, 0.56033734,\n",
      "       0.55334379, 0.55778161, 0.56984033, 0.56889929, 0.57435528,\n",
      "       0.56233255, 0.56395238, 0.57269946, 0.56746458, 0.56044532,\n",
      "       0.55626462, 0.56090813, 0.56311933, 0.56783483, 0.55570925,\n",
      "       0.5553133 , 0.56088756, 0.55942715, 0.55608464, 0.55328208,\n",
      "       0.55776104, 0.56217314, 0.56381868, 0.5516674 , 0.55658345,\n",
      "       0.55075206, 0.5595557 , 0.55220734, 0.55461394, 0.57031857,\n",
      "       0.57068367, 0.57345538, 0.56359242, 0.56423007, 0.57038542,\n",
      "       0.56801995, 0.56233255, 0.55366776, 0.56129895, 0.56302677,\n",
      "       0.57299257, 0.55307639, 0.55926259, 0.55908775, 0.56298563,\n",
      "       0.55579667, 0.55227933, 0.55762219, 0.56184146, 0.56619443,\n",
      "       0.55034068, 0.555447  , 0.54807806, 0.55628519, 0.55753992,\n",
      "       0.55732908, 0.56801481, 0.56951379, 0.57286915, 0.56491399,\n",
      "       0.56429692, 0.57185098, 0.56373641, 0.5630679 , 0.55596637,\n",
      "       0.55947343, 0.56590646, 0.57131618, 0.5514617 , 0.55471164,\n",
      "       0.56536652, 0.56252282, 0.5576839 , 0.55562183, 0.55791016,\n",
      "       0.56329417, 0.56441005, 0.54810377, 0.55477849, 0.55378089,\n",
      "       0.560795  , 0.55415113, 0.55810557, 0.56788111, 0.56552079,\n",
      "       0.57336796, 0.56261024, 0.56691435, 0.57288972, 0.56513511,\n",
      "       0.56388039, 0.57532204, 0.56587047, 0.56730003, 0.57309284,\n",
      "       0.55793588, 0.5605276 , 0.57078652, 0.56263595, 0.55900548,\n",
      "       0.57365593, 0.56271823, 0.56644126, 0.57091507, 0.55981282,\n",
      "       0.56978891, 0.56674466, 0.55891291, 0.5606613 , 0.56660067,\n",
      "       0.56806623, 0.57081737, 0.57087908, 0.55384259, 0.56143779,\n",
      "       0.57176356, 0.56175147, 0.56463631, 0.55469108, 0.55973569,\n",
      "       0.57135731, 0.56854447, 0.55143599, 0.55265986, 0.56519682,\n",
      "       0.56059445, 0.56471344, 0.55546756, 0.55671715, 0.56595274,\n",
      "       0.56997917, 0.55299411, 0.55513331, 0.55577096, 0.55551385,\n",
      "       0.56417864, 0.55574525, 0.56683722, 0.57283316, 0.56753143,\n",
      "       0.55570411, 0.55831126, 0.56941866, 0.56278508, 0.564842  ,\n",
      "       0.55696912, 0.5570154 , 0.57152187, 0.56803024, 0.55748335,\n",
      "       0.55855295, 0.56534595, 0.56062531, 0.56369012, 0.55546756,\n",
      "       0.56223999, 0.56312447, 0.56408094, 0.55033554, 0.55816214,\n",
      "       0.55685085, 0.55449053, 0.56302677, 0.56233255, 0.56817422,\n",
      "       0.57255547, 0.56951122, 0.55586095, 0.56200345, 0.56991747,\n",
      "       0.56111383, 0.56057388, 0.55565783, 0.55556013, 0.56920268,\n",
      "       0.5725709 , 0.55614635, 0.5577199 , 0.56120639, 0.55667087,\n",
      "       0.56048132, 0.55484534, 0.56538195, 0.56171033, 0.57170699,\n",
      "       0.55642404, 0.55803872, 0.55263415, 0.5596894 , 0.56569563,\n",
      "       0.55951457, 0.56889929, 0.5713316 , 0.57512663, 0.55845525,\n",
      "       0.56218857, 0.5713676 , 0.56410665, 0.56190574, 0.55703083,\n",
      "       0.56088756, 0.57128018, 0.57118247, 0.5514617 , 0.55992081,\n",
      "       0.56008536, 0.56046589, 0.56363356, 0.55555498, 0.55247474,\n",
      "       0.56220914, 0.5736405 , 0.55461908, 0.55552927, 0.55926259,\n",
      "       0.55556013, 0.55837297, 0.55809529, 0.56751601, 0.57060654,\n",
      "       0.57526033, 0.56029105, 0.56538195, 0.577019  , 0.56141722,\n",
      "       0.56904841, 0.5798987 , 0.55896434, 0.56838506, 0.57612424,\n",
      "       0.55335922, 0.5640398 , 0.57562543, 0.56740287, 0.57114648,\n",
      "       0.57962101, 0.56600417, 0.5663487 , 0.57558944, 0.55466022,\n",
      "       0.56089271, 0.57102049, 0.55738565, 0.56399866, 0.57399018]), 'split1_test_score': array([0.59829275, 0.59933664, 0.60119302, 0.6045818 , 0.59807163,\n",
      "       0.59460057, 0.60026226, 0.59367495, 0.59357211, 0.59464171,\n",
      "       0.5931093 , 0.60335279, 0.59051757, 0.59350012, 0.60103361,\n",
      "       0.59723344, 0.59587072, 0.58425424, 0.59668321, 0.59204998,\n",
      "       0.59939835, 0.58841951, 0.59713059, 0.59181344, 0.59482683,\n",
      "       0.59243051, 0.59075412, 0.59903839, 0.599964  , 0.60265858,\n",
      "       0.6005708 , 0.59758311, 0.59880698, 0.59940349, 0.5958193 ,\n",
      "       0.59299617, 0.59468285, 0.59140719, 0.59739799, 0.58730363,\n",
      "       0.59497596, 0.59966061, 0.58921143, 0.59248708, 0.58850179,\n",
      "       0.59615869, 0.59913609, 0.58877433, 0.58653228, 0.5909701 ,\n",
      "       0.59360296, 0.58817268, 0.5914689 , 0.5859872 , 0.59999486,\n",
      "       0.6002314 , 0.60636104, 0.60215977, 0.59663178, 0.60042681,\n",
      "       0.6019078 , 0.59523822, 0.59423547, 0.59583473, 0.59085697,\n",
      "       0.6019258 , 0.59603013, 0.60137814, 0.59409662, 0.59092382,\n",
      "       0.58852236, 0.5939475 , 0.59212455, 0.59521251, 0.60443267,\n",
      "       0.58168822, 0.59383951, 0.58636258, 0.59141748, 0.58720592,\n",
      "       0.58303551, 0.60050909, 0.59882755, 0.60225748, 0.60104903,\n",
      "       0.59838531, 0.60416527, 0.60326537, 0.59580387, 0.59141748,\n",
      "       0.59803049, 0.59039416, 0.60026226, 0.59513537, 0.59697118,\n",
      "       0.59640038, 0.59637981, 0.59140205, 0.59641581, 0.59127864,\n",
      "       0.59403492, 0.60107989, 0.58843494, 0.59673463, 0.58821382,\n",
      "       0.58095801, 0.5909701 , 0.59437431, 0.60823284, 0.60110046,\n",
      "       0.61224385, 0.60655645, 0.6029414 , 0.61493842, 0.60180238,\n",
      "       0.59778366, 0.61004808, 0.59822076, 0.60446353, 0.61070115,\n",
      "       0.59171573, 0.60103875, 0.6089322 , 0.5964878 , 0.6001337 ,\n",
      "       0.60562055, 0.59816419, 0.60027768, 0.60159926, 0.59565989,\n",
      "       0.6004011 , 0.60502918, 0.59540277, 0.59625125, 0.60201064,\n",
      "       0.59791222, 0.59719744, 0.59368524, 0.6024066 , 0.59949091,\n",
      "       0.59603013, 0.60368961, 0.60046281, 0.59506338, 0.59845216,\n",
      "       0.58737562, 0.60006685, 0.59464685, 0.59876584, 0.59500167,\n",
      "       0.59487826, 0.59174144, 0.58798756, 0.58985936, 0.59307845,\n",
      "       0.59375723, 0.58871519, 0.5948474 , 0.59558275, 0.60033939,\n",
      "       0.59201913, 0.58658885, 0.59536163, 0.59475484, 0.59860643,\n",
      "       0.60363047, 0.59868357, 0.59580387, 0.60308025, 0.60086391,\n",
      "       0.59506338, 0.59525364, 0.58971537, 0.60211863, 0.59122207,\n",
      "       0.60062736, 0.59045587, 0.5956496 , 0.59046101, 0.58113285,\n",
      "       0.59427661, 0.59356182, 0.59479598, 0.59357211, 0.60059137,\n",
      "       0.59241509, 0.59624097, 0.59160774, 0.58358574, 0.59683233,\n",
      "       0.59654437, 0.59595814, 0.60300825, 0.59582444, 0.59456457,\n",
      "       0.60357905, 0.60266372, 0.5943126 , 0.59865786, 0.59293446,\n",
      "       0.5958193 , 0.59251279, 0.59972746, 0.59714087, 0.59799193,\n",
      "       0.5965238 , 0.5876893 , 0.59355668, 0.59199342, 0.59210141,\n",
      "       0.59288304, 0.59791222, 0.59283676, 0.59632839, 0.59036845,\n",
      "       0.58613118, 0.5958193 , 0.59762425, 0.59430746, 0.60316252,\n",
      "       0.60022112, 0.60046795, 0.60379503, 0.60130872, 0.5936801 ,\n",
      "       0.59676034, 0.58759674, 0.60275114, 0.59922865, 0.59470342,\n",
      "       0.59736199, 0.59767053, 0.59187   , 0.58473761, 0.59550048,\n",
      "       0.5883578 , 0.59461086, 0.59330985, 0.59904867, 0.59373666,\n",
      "       0.59794822, 0.5873242 , 0.58647057, 0.59788651, 0.60000514,\n",
      "       0.61140565, 0.60526059, 0.60141928, 0.6133443 , 0.60479778,\n",
      "       0.60404186, 0.61270151, 0.6004371 , 0.59487826, 0.61015093,\n",
      "       0.60579025, 0.59954748, 0.61031548, 0.59961433, 0.60174839,\n",
      "       0.6022729 , 0.59731057, 0.5976911 , 0.60953385, 0.60391844,\n",
      "       0.59709974, 0.60317281, 0.59609184, 0.59369038, 0.59508909,\n",
      "       0.59666264, 0.59723344, 0.59144319, 0.59409662, 0.60180495,\n",
      "       0.59727972, 0.6037796 , 0.59980459, 0.58732934, 0.58730877,\n",
      "       0.60093076, 0.5952845 , 0.59273905, 0.59993829, 0.58852236,\n",
      "       0.59311958, 0.5973877 , 0.58470676, 0.5947137 , 0.60196436,\n",
      "       0.59018332, 0.59506338, 0.60032911, 0.58475818, 0.58824981,\n",
      "       0.58899545, 0.58071118, 0.59673977, 0.60102846, 0.59210141,\n",
      "       0.59424061, 0.60224205, 0.60152727, 0.6037796 , 0.5998303 ,\n",
      "       0.58732934, 0.59121693, 0.59316587, 0.59890983, 0.58880004,\n",
      "       0.60003085, 0.59346926, 0.59278019, 0.59580901, 0.58686139,\n",
      "       0.59050729, 0.59485254, 0.58217674, 0.58531869, 0.60330651,\n",
      "       0.59085697, 0.59189571, 0.5889646 , 0.58249042, 0.59396292,\n",
      "       0.59764996, 0.58814697, 0.59738256, 0.60201064, 0.59771167,\n",
      "       0.60476692, 0.60071992, 0.58940169, 0.59161803, 0.59777338,\n",
      "       0.59482683, 0.5935464 , 0.60389787, 0.59428689, 0.59681177,\n",
      "       0.59442573, 0.58485589, 0.58848636, 0.60562569, 0.59090839,\n",
      "       0.59269277, 0.60491605, 0.58780757, 0.59920294, 0.59223511,\n",
      "       0.5766539 , 0.59576273, 0.60044738, 0.59008048, 0.59715116,\n",
      "       0.60199522, 0.60071992, 0.60293626, 0.59825161, 0.58933998,\n",
      "       0.58613118, 0.59661122, 0.59171059, 0.58635744, 0.60251459,\n",
      "       0.59172602, 0.59485254, 0.59614841, 0.58660427, 0.59002391,\n",
      "       0.5982619 , 0.58555524, 0.59048158, 0.60413442, 0.59186486,\n",
      "       0.59357211, 0.58955596, 0.58023809, 0.59764482, 0.60647932,\n",
      "       0.61146736, 0.59990744, 0.60574911, 0.60945671, 0.60084848,\n",
      "       0.60256601, 0.60995038, 0.5954182 , 0.60039596, 0.60778546,\n",
      "       0.59984059, 0.60211863, 0.60865451, 0.59696604, 0.59640038,\n",
      "       0.61161649, 0.59503767, 0.61393053, 0.60569768, 0.59596328,\n",
      "       0.60804258, 0.60961098, 0.58933998, 0.59636953, 0.60962641]), 'split2_test_score': array([0.59153649, 0.5885972 , 0.58421667, 0.59012108, 0.58878316,\n",
      "       0.58643793, 0.58739875, 0.58558042, 0.58975432, 0.58624163,\n",
      "       0.59121622, 0.58672204, 0.58643793, 0.58964067, 0.5871198 ,\n",
      "       0.57864803, 0.59082879, 0.59494586, 0.59124204, 0.58260497,\n",
      "       0.58395838, 0.58432515, 0.58638111, 0.58067816, 0.57883916,\n",
      "       0.5831422 , 0.57865319, 0.59098376, 0.59070481, 0.58353996,\n",
      "       0.59048268, 0.59256963, 0.58738325, 0.58676853, 0.5873316 ,\n",
      "       0.58646376, 0.58717146, 0.59315853, 0.58281676, 0.58474357,\n",
      "       0.58661356, 0.59181544, 0.57912327, 0.59402637, 0.59495619,\n",
      "       0.59274527, 0.59044652, 0.59302422, 0.58822527, 0.58327651,\n",
      "       0.59681069, 0.5813497 , 0.582419  , 0.58579221, 0.59066865,\n",
      "       0.58998678, 0.5862468 , 0.58871084, 0.59241466, 0.58791532,\n",
      "       0.58639144, 0.58670655, 0.58752531, 0.58472291, 0.58918092,\n",
      "       0.58399971, 0.58122056, 0.58858687, 0.59016758, 0.57744442,\n",
      "       0.59170696, 0.58656191, 0.58554684, 0.59190842, 0.58075048,\n",
      "       0.58376725, 0.58356579, 0.58747624, 0.57819345, 0.58555976,\n",
      "       0.57958302, 0.59231651, 0.58967167, 0.58396355, 0.59219254,\n",
      "       0.58933073, 0.58538929, 0.58792566, 0.58708364, 0.58676337,\n",
      "       0.58469708, 0.5909476 , 0.58052318, 0.5849502 , 0.58501736,\n",
      "       0.59146934, 0.57950037, 0.58836474, 0.59416584, 0.58883482,\n",
      "       0.58664456, 0.58215038, 0.58202124, 0.58585937, 0.58867468,\n",
      "       0.58152017, 0.58438714, 0.58105009, 0.59425882, 0.59229585,\n",
      "       0.5927556 , 0.59699149, 0.58872118, 0.59142801, 0.58963034,\n",
      "       0.59531779, 0.59311203, 0.59426915, 0.59036387, 0.58647409,\n",
      "       0.59464625, 0.58857137, 0.5894702 , 0.58765187, 0.59170696,\n",
      "       0.5922287 , 0.58209356, 0.59016241, 0.58163898, 0.59049818,\n",
      "       0.5925903 , 0.5849502 , 0.58846806, 0.58875217, 0.59373192,\n",
      "       0.59063766, 0.59199624, 0.58265662, 0.58537896, 0.58500703,\n",
      "       0.58521365, 0.58599368, 0.58679436, 0.58506901, 0.59047235,\n",
      "       0.59084428, 0.58358645, 0.58617448, 0.58283742, 0.58395322,\n",
      "       0.58153566, 0.58700099, 0.58435098, 0.58669105, 0.58669105,\n",
      "       0.58815811, 0.58709914, 0.59534362, 0.58079697, 0.58409269,\n",
      "       0.58979048, 0.58397388, 0.59195491, 0.59434147, 0.58087962,\n",
      "       0.58287875, 0.58441813, 0.58789983, 0.58757955, 0.58734709,\n",
      "       0.58435615, 0.59197041, 0.59271944, 0.58139102, 0.58412369,\n",
      "       0.58418051, 0.57782151, 0.58139619, 0.58542028, 0.58764671,\n",
      "       0.57844398, 0.59124204, 0.58708881, 0.58479523, 0.59268328,\n",
      "       0.58508968, 0.58289425, 0.590555  , 0.58454211, 0.59200657,\n",
      "       0.59189293, 0.58397905, 0.58423733, 0.58670655, 0.58471775,\n",
      "       0.58636561, 0.58615898, 0.58315253, 0.59189809, 0.59214605,\n",
      "       0.58408236, 0.58544611, 0.58491921, 0.57785251, 0.57803847,\n",
      "       0.58658257, 0.5836691 , 0.58709397, 0.58959935, 0.5853118 ,\n",
      "       0.58472808, 0.59135052, 0.58491921, 0.58215555, 0.58591619,\n",
      "       0.58595752, 0.58869018, 0.59119039, 0.58705265, 0.5864121 ,\n",
      "       0.58535313, 0.58356062, 0.58755889, 0.58649475, 0.58426316,\n",
      "       0.59345297, 0.5858697 , 0.58594719, 0.58435098, 0.58128771,\n",
      "       0.58196442, 0.58196958, 0.58905178, 0.58722312, 0.5865774 ,\n",
      "       0.58863336, 0.59093727, 0.58286325, 0.5882046 , 0.58559592,\n",
      "       0.58093644, 0.58975948, 0.58394289, 0.59383007, 0.59174312,\n",
      "       0.59044136, 0.5882046 , 0.58724378, 0.59461526, 0.5891396 ,\n",
      "       0.58963551, 0.59243016, 0.59076163, 0.5949252 , 0.5891551 ,\n",
      "       0.59335999, 0.58420634, 0.59061699, 0.58590586, 0.5896665 ,\n",
      "       0.5882201 , 0.59482189, 0.59446545, 0.58418568, 0.59020374,\n",
      "       0.58757439, 0.582419  , 0.58397388, 0.58961484, 0.58643793,\n",
      "       0.58917059, 0.58956835, 0.5844233 , 0.58811162, 0.58600917,\n",
      "       0.58023907, 0.58805996, 0.58882966, 0.58325068, 0.59230618,\n",
      "       0.58873667, 0.57993429, 0.58982664, 0.58290974, 0.56944272,\n",
      "       0.58467125, 0.58796182, 0.58122572, 0.58644826, 0.58963034,\n",
      "       0.58348314, 0.58207806, 0.58115857, 0.56963902, 0.58938755,\n",
      "       0.58778618, 0.57759422, 0.58853521, 0.58922225, 0.58235701,\n",
      "       0.58937722, 0.58251198, 0.57912844, 0.58805996, 0.58931523,\n",
      "       0.58325068, 0.59393855, 0.58937206, 0.57554343, 0.58710947,\n",
      "       0.58055934, 0.57429333, 0.58840607, 0.58354513, 0.58042504,\n",
      "       0.58720245, 0.58860753, 0.5804302 , 0.58025457, 0.58271861,\n",
      "       0.56798599, 0.58690284, 0.58656707, 0.57888565, 0.59061699,\n",
      "       0.58999711, 0.58138069, 0.58653091, 0.58246033, 0.5807918 ,\n",
      "       0.58871859, 0.58931523, 0.58325068, 0.59135052, 0.58866952,\n",
      "       0.57737726, 0.58467125, 0.58489338, 0.57291408, 0.58490888,\n",
      "       0.58828209, 0.57649909, 0.57914394, 0.58952703, 0.58246549,\n",
      "       0.58670138, 0.58799281, 0.573379  , 0.58883999, 0.58528597,\n",
      "       0.58078664, 0.58989379, 0.58970783, 0.58602984, 0.59035871,\n",
      "       0.58300273, 0.57743925, 0.58674271, 0.59048785, 0.57877201,\n",
      "       0.59095276, 0.58785333, 0.58327134, 0.58704748, 0.58373368,\n",
      "       0.57820894, 0.58686152, 0.58692867, 0.58033722, 0.58169064,\n",
      "       0.5835038 , 0.57730494, 0.58309054, 0.58268245, 0.57904062,\n",
      "       0.58587487, 0.58976465, 0.58199025, 0.59169663, 0.59189809,\n",
      "       0.59112323, 0.59591444, 0.58686152, 0.59228035, 0.59233201,\n",
      "       0.59388689, 0.58973882, 0.58581288, 0.59094243, 0.59341165,\n",
      "       0.58914993, 0.58635528, 0.59071514, 0.59336515, 0.59268845,\n",
      "       0.58890714, 0.59151583, 0.59623213, 0.58445429, 0.5893204 ,\n",
      "       0.58424767, 0.58687185, 0.59038454, 0.59266778, 0.58317836]), 'mean_test_score': array([0.58618395, 0.58459327, 0.58520579, 0.58667348, 0.58275107,\n",
      "       0.58329136, 0.58336585, 0.58044486, 0.57979102, 0.58080388,\n",
      "       0.58113528, 0.58720098, 0.57736637, 0.5789075 , 0.58349601,\n",
      "       0.57989585, 0.58249873, 0.57870051, 0.5823944 , 0.57909431,\n",
      "       0.58489168, 0.57693249, 0.57832352, 0.57862112, 0.57646691,\n",
      "       0.57528653, 0.57396057, 0.58774111, 0.58600492, 0.58490508,\n",
      "       0.5860888 , 0.58463743, 0.58521847, 0.5825557 , 0.58237087,\n",
      "       0.579183  , 0.58057301, 0.58119678, 0.58138453, 0.57475425,\n",
      "       0.57777133, 0.5840056 , 0.57733511, 0.58178239, 0.57971603,\n",
      "       0.58226517, 0.58395653, 0.58361025, 0.57635563, 0.5749607 ,\n",
      "       0.58106917, 0.57616689, 0.57547639, 0.57707941, 0.5865744 ,\n",
      "       0.58583798, 0.58727953, 0.58629331, 0.58358749, 0.58438347,\n",
      "       0.5832999 , 0.58073219, 0.5800921 , 0.57994847, 0.58040298,\n",
      "       0.58472445, 0.57879479, 0.5805655 , 0.58120482, 0.57750882,\n",
      "       0.57825034, 0.5795759 , 0.57798152, 0.58275857, 0.5845273 ,\n",
      "       0.57360762, 0.57647734, 0.57866399, 0.57544576, 0.57492297,\n",
      "       0.57249288, 0.58803031, 0.58585318, 0.58619881, 0.58719653,\n",
      "       0.58325409, 0.58560465, 0.58493755, 0.58121107, 0.57809566,\n",
      "       0.57982665, 0.58008354, 0.58337953, 0.5780617 , 0.57800685,\n",
      "       0.58467949, 0.57961688, 0.57761655, 0.58212901, 0.57870619,\n",
      "       0.57975265, 0.58344937, 0.57428903, 0.57823564, 0.5783424 ,\n",
      "       0.57388211, 0.57521371, 0.57664869, 0.59236145, 0.58653961,\n",
      "       0.59281661, 0.59215852, 0.58597426, 0.59387229, 0.58642009,\n",
      "       0.58719031, 0.59299185, 0.5864862 , 0.58515211, 0.58796929,\n",
      "       0.58238325, 0.58341991, 0.5905498 , 0.58240542, 0.58564201,\n",
      "       0.58930135, 0.58054173, 0.58503359, 0.58642521, 0.58033879,\n",
      "       0.58371456, 0.5847672 , 0.58230139, 0.58032547, 0.58669508,\n",
      "       0.58408019, 0.5857846 , 0.58261607, 0.58381033, 0.58317878,\n",
      "       0.58332303, 0.58571575, 0.58228984, 0.57879426, 0.58323951,\n",
      "       0.58275948, 0.58478847, 0.57867159, 0.57898642, 0.58056886,\n",
      "       0.57867323, 0.57754176, 0.57574545, 0.57764274, 0.58061724,\n",
      "       0.58389305, 0.57669014, 0.58370035, 0.57743416, 0.58158791,\n",
      "       0.57837584, 0.57610883, 0.58571424, 0.58599222, 0.5846166 ,\n",
      "       0.58294731, 0.5823498 , 0.58546588, 0.5860403 , 0.58288207,\n",
      "       0.578557  , 0.58270373, 0.58184316, 0.5837833 , 0.57701296,\n",
      "       0.58003726, 0.57638723, 0.57882237, 0.57731585, 0.57401024,\n",
      "       0.57682599, 0.58231893, 0.58189723, 0.5766721 , 0.58327892,\n",
      "       0.5760788 , 0.57956112, 0.57811396, 0.57423947, 0.58638157,\n",
      "       0.58636948, 0.58446456, 0.5836122 , 0.58225032, 0.58322145,\n",
      "       0.58598792, 0.58371657, 0.57703968, 0.58394562, 0.58269528,\n",
      "       0.58429824, 0.57700538, 0.58130035, 0.57802718, 0.57967325,\n",
      "       0.57962909, 0.57453901, 0.57941848, 0.58113835, 0.58119944,\n",
      "       0.57597732, 0.58156252, 0.57527072, 0.57825343, 0.57793549,\n",
      "       0.57646542, 0.58417135, 0.58610563, 0.58474134, 0.58482834,\n",
      "       0.58328883, 0.58529449, 0.58502819, 0.58362162, 0.57796512,\n",
      "       0.58322118, 0.57978637, 0.58667205, 0.57834257, 0.57689761,\n",
      "       0.58156401, 0.58072003, 0.57952803, 0.57585226, 0.57999104,\n",
      "       0.58008865, 0.58331363, 0.57475283, 0.58067156, 0.57769852,\n",
      "       0.57989243, 0.57706868, 0.57616713, 0.58652704, 0.58575182,\n",
      "       0.5917393 , 0.58535632, 0.58519092, 0.59361567, 0.58635539,\n",
      "       0.58584972, 0.59348537, 0.5856859 , 0.58569418, 0.59080087,\n",
      "       0.58568957, 0.58142504, 0.59057296, 0.5827163 , 0.58346877,\n",
      "       0.58804951, 0.58494276, 0.58619302, 0.58821458, 0.58464079,\n",
      "       0.58481893, 0.58411344, 0.57965628, 0.5813159 , 0.58270641,\n",
      "       0.58462972, 0.58587026, 0.58224688, 0.57867648, 0.58308176,\n",
      "       0.58309627, 0.58452768, 0.58442019, 0.57508419, 0.57977407,\n",
      "       0.58700694, 0.58125542, 0.57799162, 0.5784993 , 0.57439104,\n",
      "       0.57945782, 0.58335084, 0.5737944 , 0.57928762, 0.58584629,\n",
      "       0.5812135 , 0.57670779, 0.57887193, 0.57005637, 0.57770824,\n",
      "       0.58031444, 0.57134549, 0.584034  , 0.58769347, 0.580662  ,\n",
      "       0.57976672, 0.58102064, 0.58336132, 0.58487247, 0.58465899,\n",
      "       0.57584412, 0.58071363, 0.58468305, 0.58083183, 0.57779058,\n",
      "       0.57971374, 0.57770543, 0.58059795, 0.58101284, 0.57424666,\n",
      "       0.57997778, 0.58219   , 0.57555895, 0.57196333, 0.58139475,\n",
      "       0.57190089, 0.57775611, 0.57951415, 0.57456628, 0.58424656,\n",
      "       0.58673171, 0.57967834, 0.57991981, 0.58215791, 0.5828085 ,\n",
      "       0.58486353, 0.58353197, 0.57609799, 0.5795006 , 0.58521258,\n",
      "       0.58159485, 0.57811638, 0.58216832, 0.57613822, 0.57945972,\n",
      "       0.58105759, 0.57206342, 0.57766963, 0.58561806, 0.58169304,\n",
      "       0.57859994, 0.58364591, 0.57127198, 0.58257271, 0.58106905,\n",
      "       0.57231196, 0.58484812, 0.58716034, 0.58374392, 0.58198204,\n",
      "       0.58239505, 0.58317993, 0.58459358, 0.58354315, 0.57504479,\n",
      "       0.57931504, 0.58524627, 0.58205388, 0.5749464 , 0.58205509,\n",
      "       0.57667228, 0.58072201, 0.58223333, 0.57416082, 0.5747245 ,\n",
      "       0.5813233 , 0.57883472, 0.57605842, 0.58078061, 0.57672094,\n",
      "       0.57833   , 0.57922323, 0.57343474, 0.58561455, 0.58965962,\n",
      "       0.5926181 , 0.585363  , 0.58599687, 0.59291917, 0.58486026,\n",
      "       0.58849637, 0.59319858, 0.58006079, 0.58657118, 0.59243971,\n",
      "       0.58077692, 0.58416959, 0.59166575, 0.58590572, 0.58674061,\n",
      "       0.59338493, 0.58418034, 0.59216738, 0.58858359, 0.57997424,\n",
      "       0.58439443, 0.58916951, 0.57902814, 0.58433903, 0.588936  ]), 'std_test_score': array([0.01264971, 0.01396335, 0.01267468, 0.0162163 , 0.01556626,\n",
      "       0.01075177, 0.01570487, 0.01339933, 0.01685555, 0.01405356,\n",
      "       0.01560703, 0.01300213, 0.01579625, 0.01797016, 0.01600736,\n",
      "       0.01368139, 0.01547843, 0.01600347, 0.01650513, 0.01226601,\n",
      "       0.01148776, 0.01344989, 0.01949422, 0.01170427, 0.01605082,\n",
      "       0.01807689, 0.01597678, 0.01079474, 0.01372446, 0.01397819,\n",
      "       0.01396848, 0.01489973, 0.01207844, 0.01576141, 0.01346982,\n",
      "       0.01514894, 0.01495828, 0.01568408, 0.01370087, 0.01596452,\n",
      "       0.01872785, 0.01689527, 0.01050546, 0.01622889, 0.0171773 ,\n",
      "       0.01728397, 0.01572616, 0.01044404, 0.0155942 , 0.0174813 ,\n",
      "       0.02002261, 0.01246777, 0.01663105, 0.01245299, 0.01295703,\n",
      "       0.01376274, 0.0151826 , 0.01404904, 0.0155556 , 0.01475582,\n",
      "       0.01660297, 0.01489331, 0.01549761, 0.01529777, 0.01360986,\n",
      "       0.01376435, 0.01516347, 0.02104534, 0.01553092, 0.0109313 ,\n",
      "       0.01681691, 0.01539746, 0.01557977, 0.01532961, 0.01495961,\n",
      "       0.01291671, 0.01778897, 0.01167626, 0.01429773, 0.01621276,\n",
      "       0.01254336, 0.0123171 , 0.01244913, 0.01230766, 0.01380902,\n",
      "       0.01544372, 0.01507343, 0.01632484, 0.01490133, 0.01565952,\n",
      "       0.01720107, 0.01496554, 0.01278617, 0.01744498, 0.01900321,\n",
      "       0.01323949, 0.01364482, 0.01738479, 0.01862752, 0.01607592,\n",
      "       0.01527075, 0.0139017 , 0.01568652, 0.01899574, 0.01427894,\n",
      "       0.01040053, 0.01782699, 0.01656614, 0.01380219, 0.01480798,\n",
      "       0.01584347, 0.01414762, 0.01510329, 0.01630261, 0.01405652,\n",
      "       0.01327033, 0.01398085, 0.01389027, 0.01827199, 0.01798928,\n",
      "       0.01530612, 0.0168869 , 0.01459487, 0.01413475, 0.0149357 ,\n",
      "       0.01466888, 0.01506638, 0.01498576, 0.01097745, 0.01813415,\n",
      "       0.01835034, 0.01662517, 0.01391337, 0.01748605, 0.01616045,\n",
      "       0.01471785, 0.01263906, 0.00905793, 0.01586744, 0.01412794,\n",
      "       0.0112293 , 0.0147957 , 0.01698   , 0.01645363, 0.01619958,\n",
      "       0.00908405, 0.01201969, 0.01695535, 0.01793262, 0.01338305,\n",
      "       0.01454393, 0.01683525, 0.01481063, 0.01508612, 0.01336076,\n",
      "       0.0102478 , 0.01586929, 0.01610694, 0.01636739, 0.01643246,\n",
      "       0.01773372, 0.01301051, 0.01131667, 0.01209457, 0.01024991,\n",
      "       0.01686625, 0.01425934, 0.00959123, 0.01458611, 0.01680561,\n",
      "       0.01636635, 0.01547868, 0.01330389, 0.01410318, 0.01534849,\n",
      "       0.01873633, 0.01211819, 0.01490463, 0.0151615 , 0.0149044 ,\n",
      "       0.01495678, 0.01428472, 0.01317189, 0.01805012, 0.0191611 ,\n",
      "       0.01816662, 0.01516619, 0.01833418, 0.01389071, 0.01153414,\n",
      "       0.01125906, 0.00919658, 0.01610359, 0.01328156, 0.00993123,\n",
      "       0.01452488, 0.01656145, 0.01715177, 0.01625853, 0.01391935,\n",
      "       0.00932374, 0.01717391, 0.01672246, 0.01554149, 0.01434322,\n",
      "       0.017343  , 0.01583416, 0.01564504, 0.01368763, 0.01097192,\n",
      "       0.01844102, 0.01867004, 0.01950856, 0.01658436, 0.01454401,\n",
      "       0.0135393 , 0.01179544, 0.01202902, 0.00890635, 0.0156608 ,\n",
      "       0.01474381, 0.01175124, 0.0164574 , 0.01574894, 0.01603188,\n",
      "       0.01685587, 0.00984541, 0.01284833, 0.01996441, 0.01662448,\n",
      "       0.0130701 , 0.01438153, 0.01549764, 0.0143491 , 0.01604133,\n",
      "       0.01188276, 0.01345823, 0.01933057, 0.01884668, 0.01724507,\n",
      "       0.01519138, 0.01624479, 0.01282028, 0.01329564, 0.01470551,\n",
      "       0.01556172, 0.01753431, 0.0141663 , 0.01653684, 0.01631727,\n",
      "       0.01661858, 0.01528406, 0.0145652 , 0.01301403, 0.01517919,\n",
      "       0.02028086, 0.01605645, 0.01614376, 0.01526917, 0.01799682,\n",
      "       0.01168787, 0.01575681, 0.01403641, 0.01602666, 0.0184364 ,\n",
      "       0.01132233, 0.01492541, 0.01548718, 0.01470767, 0.01192953,\n",
      "       0.01211123, 0.01110008, 0.00853796, 0.01773919, 0.01661521,\n",
      "       0.01061447, 0.01734477, 0.01469661, 0.01452402, 0.01432331,\n",
      "       0.01213951, 0.01096049, 0.01882589, 0.01955831, 0.01014711,\n",
      "       0.01378429, 0.01373592, 0.013044  , 0.01632128, 0.01494812,\n",
      "       0.0084057 , 0.01759491, 0.01852861, 0.01184213, 0.01570958,\n",
      "       0.01142687, 0.01111038, 0.01261932, 0.01156555, 0.01010561,\n",
      "       0.01713983, 0.01797234, 0.01344974, 0.01689296, 0.01466289,\n",
      "       0.01345758, 0.01680345, 0.00943952, 0.01315283, 0.01438409,\n",
      "       0.01695021, 0.01173575, 0.01424315, 0.01323881, 0.0135436 ,\n",
      "       0.01262192, 0.01372775, 0.00815201, 0.01544082, 0.01846078,\n",
      "       0.01416078, 0.01658627, 0.01170593, 0.00877963, 0.01145303,\n",
      "       0.01050496, 0.00770528, 0.01758868, 0.01634044, 0.01144025,\n",
      "       0.01803477, 0.01689692, 0.0146779 , 0.01693843, 0.01192107,\n",
      "       0.00956508, 0.01596052, 0.01895717, 0.01370074, 0.01683938,\n",
      "       0.01477227, 0.01265046, 0.00949319, 0.01814641, 0.00786086,\n",
      "       0.01587907, 0.01938945, 0.0144418 , 0.01673355, 0.01124058,\n",
      "       0.00921005, 0.01153541, 0.01202632, 0.00631701, 0.01687471,\n",
      "       0.01626281, 0.01265473, 0.0159306 , 0.01563319, 0.01345519,\n",
      "       0.01318529, 0.01050786, 0.00842772, 0.01661799, 0.01743577,\n",
      "       0.01296763, 0.01469785, 0.013687  , 0.01341002, 0.01610517,\n",
      "       0.01480437, 0.00498453, 0.01546559, 0.01989587, 0.01341528,\n",
      "       0.01641337, 0.01475197, 0.01087633, 0.01303301, 0.0147356 ,\n",
      "       0.01482469, 0.01781332, 0.01649736, 0.01325533, 0.01694649,\n",
      "       0.0142083 , 0.01251409, 0.01543224, 0.01343296, 0.01294874,\n",
      "       0.01988298, 0.01562793, 0.01350586, 0.01317316, 0.01113651,\n",
      "       0.01344433, 0.01293989, 0.01964348, 0.01263744, 0.01811395,\n",
      "       0.01925641, 0.01584379, 0.01531817, 0.01447007, 0.01511141]), 'rank_test_score': array([ 55, 115,  88,  40, 175, 159, 153, 247, 265, 232, 225,  32, 338,\n",
      "       292, 148, 262, 183, 299, 186, 289,  96, 346, 312, 304, 356, 374,\n",
      "       393,  29,  59,  95,  57, 111,  86, 182, 188, 288, 243, 223, 214,\n",
      "       382, 327, 131, 339, 206, 270, 193, 132, 144, 359, 379, 226, 361,\n",
      "       372, 341,  42,  69,  31,  52, 145, 121, 158, 235, 252, 260, 248,\n",
      "       106, 296, 245, 221, 336, 314, 277, 323, 174, 117, 396, 355, 303,\n",
      "       373, 381, 398,  27,  66,  53,  33, 162,  80,  94, 220, 318, 264,\n",
      "       254, 152, 319, 321, 108, 276, 334, 200, 298, 269, 150, 388, 315,\n",
      "       310, 394, 376, 354,  11,  44,   8,  13,  63,   1,  48,  34,   6,\n",
      "        46,  90,  28, 187, 151,  18, 184,  77,  20, 246,  91,  47, 249,\n",
      "       139, 104, 191, 250,  39, 129,  70, 180, 135, 167, 156,  72, 192,\n",
      "       297, 163, 173, 103, 302, 291, 244, 301, 335, 370, 333, 241, 134,\n",
      "       351, 140, 337, 209, 308, 363,  73,  61, 113, 170, 189,  81,  58,\n",
      "       171, 306, 178, 205, 136, 344, 256, 358, 295, 340, 392, 348, 190,\n",
      "       204, 353, 161, 365, 278, 317, 390,  49,  50, 118, 143, 194, 164,\n",
      "        62, 138, 343, 133, 179, 123, 345, 217, 320, 273, 275, 386, 284,\n",
      "       224, 222, 367, 211, 375, 313, 325, 357, 126,  56, 105, 101, 160,\n",
      "        84,  92, 142, 324, 165, 266,  41, 309, 347, 210, 237, 279, 368,\n",
      "       257, 253, 157, 383, 239, 331, 263, 342, 360,  45,  71,  14,  83,\n",
      "        89,   2,  51,  67,   3,  76,  74,  16,  75, 212,  17, 176, 149,\n",
      "        26,  93,  54,  25, 110, 102, 128, 274, 216, 177, 112,  65, 195,\n",
      "       300, 169, 168, 116, 119, 377, 267,  36, 218, 322, 307, 387, 283,\n",
      "       155, 395, 286,  68, 219, 350, 293, 405, 329, 251, 403, 130,  30,\n",
      "       240, 268, 229, 154,  97, 109, 369, 238, 107, 231, 326, 271, 330,\n",
      "       242, 230, 389, 258, 197, 371, 401, 213, 402, 328, 280, 385, 124,\n",
      "        38, 272, 261, 199, 172,  98, 147, 364, 281,  87, 208, 316, 198,\n",
      "       362, 282, 228, 400, 332,  78, 207, 305, 141, 404, 181, 227, 399,\n",
      "       100,  35, 137, 203, 185, 166, 114, 146, 378, 285,  85, 202, 380,\n",
      "       201, 352, 236, 196, 391, 384, 215, 294, 366, 233, 349, 311, 287,\n",
      "       397,  79,  19,   9,  82,  60,   7,  99,  24,   5, 255,  43,  10,\n",
      "       234, 127,  15,  64,  37,   4, 125,  12,  23, 259, 120,  21, 290,\n",
      "       122,  22]), 'split0_train_score': array([0.94871131, 0.95260773, 0.93414998, 0.93005127, 0.93752843,\n",
      "       0.92895089, 0.90625785, 0.91640091, 0.9072938 , 0.99499676,\n",
      "       0.99691533, 0.99049217, 0.98129873, 0.98853494, 0.98212595,\n",
      "       0.9633319 , 0.97125746, 0.96250211, 0.99996135, 0.99999485,\n",
      "       0.99949233, 0.99758278, 0.99887772, 0.99758922, 0.98567963,\n",
      "       0.99162862, 0.98854138, 0.94757356, 0.95197121, 0.93715219,\n",
      "       0.92988441, 0.93744725, 0.92785373, 0.90651684, 0.91587521,\n",
      "       0.90665084, 0.99426876, 0.99651461, 0.99126527, 0.98208085,\n",
      "       0.98814452, 0.98349047, 0.96409727, 0.97093147, 0.96430987,\n",
      "       0.99997165, 0.99999227, 0.99975261, 0.99777347, 0.99878366,\n",
      "       0.99834041, 0.9860855 , 0.9915075 , 0.98871791, 0.9455751 ,\n",
      "       0.9523423 , 0.9337789 , 0.92764435, 0.93736994, 0.92898182,\n",
      "       0.90722423, 0.91477612, 0.90403004, 0.99390282, 0.99630459,\n",
      "       0.99019581, 0.98184763, 0.98848211, 0.98343635, 0.96267348,\n",
      "       0.97192748, 0.96311801, 0.99991496, 0.99998196, 0.99954774,\n",
      "       0.9977477 , 0.99879268, 0.99833268, 0.98509594, 0.99158997,\n",
      "       0.98785719, 0.94684685, 0.95192611, 0.92929492, 0.92907716,\n",
      "       0.9374872 , 0.93011441, 0.90619343, 0.91552989, 0.90814293,\n",
      "       0.99326502, 0.99545418, 0.98915857, 0.98181799, 0.98813679,\n",
      "       0.98165049, 0.96147904, 0.9714211 , 0.96500953, 0.99977709,\n",
      "       0.99990981, 0.99907486, 0.99685477, 0.99861358, 0.99754799,\n",
      "       0.98473516, 0.99113126, 0.9862311 , 0.91688281, 0.9139154 ,\n",
      "       0.83196688, 0.9004712 , 0.90251347, 0.83580918, 0.88283423,\n",
      "       0.88556327, 0.80873009, 0.96342596, 0.96684822, 0.91875114,\n",
      "       0.95175861, 0.95737131, 0.91391798, 0.92855275, 0.93606212,\n",
      "       0.88970322, 0.98267742, 0.98613704, 0.96320048, 0.97201896,\n",
      "       0.97775536, 0.96012096, 0.95095072, 0.96097524, 0.94329704,\n",
      "       0.95196863, 0.95254975, 0.9380129 , 0.93333372, 0.93858886,\n",
      "       0.92936063, 0.91157677, 0.91392184, 0.90901009, 0.9962247 ,\n",
      "       0.99654553, 0.9916647 , 0.98465785, 0.98859292, 0.98427258,\n",
      "       0.96649001, 0.97396717, 0.96335767, 0.99999742, 0.99999098,\n",
      "       0.99975905, 0.99816647, 0.99898853, 0.99854271, 0.98706605,\n",
      "       0.9930434 , 0.98915986, 0.95226112, 0.95191194, 0.93983097,\n",
      "       0.93402629, 0.93928465, 0.92927559, 0.91157677, 0.91478514,\n",
      "       0.90901009, 0.99584717, 0.99628784, 0.99123305, 0.98362962,\n",
      "       0.9883043 , 0.98599273, 0.9648111 , 0.97432022, 0.96523888,\n",
      "       0.99999613, 0.9999781 , 0.99962891, 0.99822316, 0.99910836,\n",
      "       0.99827599, 0.98755826, 0.99303438, 0.99037234, 0.95205754,\n",
      "       0.95177922, 0.93853732, 0.93266306, 0.93852959, 0.93001906,\n",
      "       0.91166826, 0.91543583, 0.90742008, 0.99566807, 0.99575569,\n",
      "       0.99061844, 0.98406514, 0.98765747, 0.98358581, 0.9653819 ,\n",
      "       0.97309615, 0.96496314, 0.99997423, 0.99998196, 0.99948718,\n",
      "       0.99809818, 0.99906326, 0.99810848, 0.9866576 , 0.99270194,\n",
      "       0.98947297, 0.95110405, 0.95154085, 0.92187897, 0.93060017,\n",
      "       0.93902437, 0.92912097, 0.91267329, 0.91400688, 0.90674877,\n",
      "       0.99483312, 0.99526734, 0.98953868, 0.98331652, 0.98740492,\n",
      "       0.98339125, 0.96482527, 0.97269542, 0.96460494, 0.99993815,\n",
      "       0.99997036, 0.99897693, 0.99757505, 0.99890735, 0.99759953,\n",
      "       0.98712146, 0.99234374, 0.98935056, 0.92462025, 0.91757216,\n",
      "       0.81259301, 0.90792903, 0.90677454, 0.8068302 , 0.88910793,\n",
      "       0.88811966, 0.7958902 , 0.96942779, 0.96764837, 0.91676942,\n",
      "       0.95722184, 0.95681983, 0.90694591, 0.93570005, 0.93959389,\n",
      "       0.87308159, 0.98755568, 0.98721423, 0.9529466 , 0.97861479,\n",
      "       0.9813361 , 0.94406369, 0.95902832, 0.96454438, 0.93052028,\n",
      "       0.95651703, 0.95676056, 0.93908751, 0.93708776, 0.9403831 ,\n",
      "       0.93063496, 0.91613162, 0.92182163, 0.90841093, 0.99714082,\n",
      "       0.99692951, 0.99289779, 0.98651715, 0.98929902, 0.98532013,\n",
      "       0.96844595, 0.97506368, 0.96917653, 1.        , 0.99998969,\n",
      "       0.99943177, 0.99844221, 0.99931581, 0.99859682, 0.9890542 ,\n",
      "       0.99385773, 0.99073698, 0.9558715 , 0.95625031, 0.93770753,\n",
      "       0.9363662 , 0.93881306, 0.93094033, 0.91596669, 0.92139642,\n",
      "       0.90814679, 0.99694884, 0.99649013, 0.99240945, 0.98681222,\n",
      "       0.98895112, 0.98540131, 0.96984398, 0.9749387 , 0.96917653,\n",
      "       0.99999356, 0.99999356, 0.99942791, 0.99834299, 0.99938281,\n",
      "       0.9980389 , 0.98871919, 0.99367734, 0.99170336, 0.95531615,\n",
      "       0.95578259, 0.9372398 , 0.93525552, 0.93922925, 0.93204715,\n",
      "       0.91562781, 0.92147438, 0.90794836, 0.99670402, 0.99684318,\n",
      "       0.992001  , 0.98713563, 0.98858906, 0.9848305 , 0.96979244,\n",
      "       0.9756216 , 0.96934662, 0.99998067, 0.99999227, 0.99947816,\n",
      "       0.99841257, 0.99931838, 0.99733538, 0.98897689, 0.99342994,\n",
      "       0.99028859, 0.95451084, 0.95446703, 0.91394504, 0.93755549,\n",
      "       0.93901149, 0.93205166, 0.91664959, 0.92256767, 0.90387799,\n",
      "       0.99593221, 0.99639607, 0.99116734, 0.98655838, 0.98804531,\n",
      "       0.98383449, 0.9688106 , 0.97537421, 0.96863923, 0.99994717,\n",
      "       0.99997552, 0.99714855, 0.99836618, 0.99906455, 0.99521967,\n",
      "       0.98928871, 0.99331011, 0.98974226, 0.93083339, 0.92337942,\n",
      "       0.79130057, 0.91368862, 0.91237564, 0.80299885, 0.8985507 ,\n",
      "       0.89807331, 0.7671715 , 0.97439495, 0.97374297, 0.85274392,\n",
      "       0.96334608, 0.96265158, 0.86910013, 0.94330992, 0.94642165,\n",
      "       0.81570216, 0.98934283, 0.98955028, 0.9228215 , 0.98175743,\n",
      "       0.98385769, 0.92046484, 0.96561512, 0.96955664, 0.8815612 ]), 'split1_train_score': array([0.95349293, 0.95170707, 0.92766497, 0.93625926, 0.93744596,\n",
      "       0.92743819, 0.91414733, 0.91954614, 0.91090933, 0.99536527,\n",
      "       0.99678906, 0.98626847, 0.98538714, 0.98994198, 0.98390279,\n",
      "       0.96687527, 0.97553656, 0.96896264, 0.99994846, 0.99999871,\n",
      "       0.99984538, 0.99793711, 0.99923592, 0.99843963, 0.9870609 ,\n",
      "       0.99359745, 0.99095732, 0.95388205, 0.95200085, 0.92493722,\n",
      "       0.93606405, 0.93824483, 0.92703489, 0.91360229, 0.91923303,\n",
      "       0.91080239, 0.99535496, 0.99678262, 0.9888532 , 0.98497482,\n",
      "       0.98948843, 0.98499801, 0.96708143, 0.97461529, 0.96846657,\n",
      "       0.99992011, 0.99999227, 0.99981059, 0.99816002, 0.99902074,\n",
      "       0.99896405, 0.98805175, 0.99329208, 0.99097664, 0.95260515,\n",
      "       0.95082316, 0.92691377, 0.93622704, 0.9384278 , 0.92666123,\n",
      "       0.9136796 , 0.91989661, 0.9089302 , 0.99524931, 0.99588196,\n",
      "       0.9880801 , 0.98506888, 0.98909157, 0.98502378, 0.96675287,\n",
      "       0.97420554, 0.97056553, 0.99987502, 0.9999884 , 0.99975132,\n",
      "       0.99794227, 0.99913799, 0.99835459, 0.98795254, 0.99374434,\n",
      "       0.99128073, 0.95135917, 0.9494625 , 0.92445661, 0.93454169,\n",
      "       0.9369267 , 0.92167474, 0.91350695, 0.92042232, 0.90974711,\n",
      "       0.99442982, 0.99521838, 0.98708924, 0.98454188, 0.98892922,\n",
      "       0.98068798, 0.9655507 , 0.9738409 , 0.96698866, 0.9998209 ,\n",
      "       0.99993429, 0.99972168, 0.99735214, 0.99890349, 0.99810075,\n",
      "       0.98682252, 0.99284239, 0.9911042 , 0.91993527, 0.91309978,\n",
      "       0.83054438, 0.90891216, 0.90379875, 0.8300934 , 0.88741098,\n",
      "       0.8868711 , 0.79903543, 0.96594627, 0.96868304, 0.91378655,\n",
      "       0.95467835, 0.9570363 , 0.91943791, 0.93544621, 0.93769851,\n",
      "       0.90129584, 0.98587419, 0.98776313, 0.96608929, 0.97733531,\n",
      "       0.98145979, 0.96105513, 0.9563972 , 0.96500051, 0.94860694,\n",
      "       0.95503526, 0.95310122, 0.93276098, 0.93873704, 0.94409719,\n",
      "       0.92829762, 0.91832207, 0.92216437, 0.91272483, 0.99607008,\n",
      "       0.99699651, 0.98802469, 0.98803371, 0.99065581, 0.98395304,\n",
      "       0.9714546 , 0.97638311, 0.96820758, 0.99996392, 0.99999613,\n",
      "       0.99982992, 0.99880427, 0.99938539, 0.9984989 , 0.98930288,\n",
      "       0.9943654 , 0.9922806 , 0.95491929, 0.95295305, 0.92928204,\n",
      "       0.93870612, 0.94364493, 0.92552348, 0.9186429 , 0.92206773,\n",
      "       0.91272483, 0.99612935, 0.99674912, 0.98841124, 0.98691916,\n",
      "       0.99033239, 0.98734694, 0.97108995, 0.97671812, 0.96665108,\n",
      "       0.99994588, 0.99997294, 0.99977451, 0.99851307, 0.99934802,\n",
      "       0.99884808, 0.98973324, 0.99471973, 0.99155131, 0.95464484,\n",
      "       0.952868  , 0.93007189, 0.93937356, 0.94501461, 0.92865969,\n",
      "       0.91864032, 0.92088875, 0.91662769, 0.99562297, 0.99694497,\n",
      "       0.98877589, 0.98602108, 0.98960053, 0.98633547, 0.9716221 ,\n",
      "       0.97741391, 0.97008492, 0.99993042, 0.99998196, 0.99992656,\n",
      "       0.99821801, 0.99936864, 0.9986896 , 0.98845892, 0.99365286,\n",
      "       0.99142246, 0.95440003, 0.95375192, 0.92938254, 0.9376212 ,\n",
      "       0.94348387, 0.91563168, 0.91909774, 0.92022518, 0.90875367,\n",
      "       0.99549799, 0.99633036, 0.9869256 , 0.98622853, 0.98927711,\n",
      "       0.98428032, 0.97050111, 0.97516934, 0.96774372, 0.99985053,\n",
      "       0.99995104, 0.99968689, 0.99807369, 0.99919598, 0.99887643,\n",
      "       0.98888412, 0.99357941, 0.99046382, 0.92986701, 0.92369768,\n",
      "       0.82473454, 0.91506474, 0.91236662, 0.8284248 , 0.89970584,\n",
      "       0.89525665, 0.79438975, 0.97321855, 0.97269027, 0.89183697,\n",
      "       0.96133731, 0.95983363, 0.90871244, 0.94354056, 0.9443201 ,\n",
      "       0.88555425, 0.98829785, 0.99004506, 0.95468221, 0.9796997 ,\n",
      "       0.98424037, 0.95443095, 0.96219674, 0.96967647, 0.93602604,\n",
      "       0.95931823, 0.95614981, 0.9283118 , 0.93971243, 0.94647061,\n",
      "       0.92741242, 0.91828212, 0.92366032, 0.91074827, 0.99704418,\n",
      "       0.99769488, 0.98624399, 0.98795383, 0.99103978, 0.98727092,\n",
      "       0.97032974, 0.97785071, 0.96759555, 0.9999884 , 1.        ,\n",
      "       0.99988017, 0.99884422, 0.99956835, 0.99896791, 0.98952708,\n",
      "       0.99506763, 0.9925499 , 0.95845365, 0.95633793, 0.92920988,\n",
      "       0.94073292, 0.9454862 , 0.92874473, 0.91828212, 0.92369768,\n",
      "       0.91074827, 0.99689987, 0.99748614, 0.98907224, 0.98773607,\n",
      "       0.99079368, 0.98587934, 0.96947804, 0.97789323, 0.96784165,\n",
      "       0.99996006, 0.99999742, 0.9998879 , 0.99863419, 0.99949233,\n",
      "       0.99896018, 0.98926423, 0.99437957, 0.99267488, 0.95809931,\n",
      "       0.95620522, 0.92822804, 0.93980392, 0.94690741, 0.92683131,\n",
      "       0.91950104, 0.92534567, 0.91296062, 0.99637545, 0.99748485,\n",
      "       0.98918434, 0.98765232, 0.99064679, 0.98650555, 0.9705694 ,\n",
      "       0.97794477, 0.96878612, 0.99996135, 0.99999742, 0.99932998,\n",
      "       0.99840226, 0.99949104, 0.99868702, 0.98884418, 0.99458702,\n",
      "       0.99250351, 0.95770889, 0.95473762, 0.92519234, 0.94039405,\n",
      "       0.94590625, 0.91405971, 0.9195165 , 0.92382524, 0.91238982,\n",
      "       0.99545418, 0.99718205, 0.98811618, 0.98779147, 0.99028536,\n",
      "       0.98108226, 0.96876872, 0.97808779, 0.97021635, 0.99991882,\n",
      "       0.99997423, 0.99801829, 0.99833397, 0.99917021, 0.99647853,\n",
      "       0.98876171, 0.99473519, 0.99154487, 0.93542817, 0.92646022,\n",
      "       0.77712064, 0.92036047, 0.91756314, 0.7612064 , 0.90054594,\n",
      "       0.90132032, 0.78545723, 0.97718326, 0.97799115, 0.86547558,\n",
      "       0.96486651, 0.96828103, 0.87391653, 0.94617683, 0.95170063,\n",
      "       0.81683089, 0.98955929, 0.992001  , 0.91260371, 0.98260913,\n",
      "       0.98596052, 0.88885152, 0.96590117, 0.97439624, 0.86134207]), 'split2_train_score': array([0.95107346, 0.94848816, 0.91600159, 0.93689996, 0.9369141 ,\n",
      "       0.92131875, 0.91307433, 0.92101214, 0.91209858, 0.99582573,\n",
      "       0.99554161, 0.98406911, 0.98651043, 0.98790785, 0.98023038,\n",
      "       0.96656468, 0.97554573, 0.972945  , 0.99994472, 0.99995115,\n",
      "       0.9995629 , 0.99838274, 0.9989574 , 0.99801635, 0.98709022,\n",
      "       0.9937868 , 0.9934924 , 0.95080863, 0.94786594, 0.91529324,\n",
      "       0.93566709, 0.93661327, 0.91847376, 0.91287121, 0.92195639,\n",
      "       0.9098951 , 0.99576145, 0.99503253, 0.98825367, 0.98651557,\n",
      "       0.9880904 , 0.98212146, 0.96585247, 0.97565886, 0.97277788,\n",
      "       0.9999023 , 0.99997172, 0.99954233, 0.99823362, 0.99917209,\n",
      "       0.99848173, 0.98757617, 0.99348597, 0.99241766, 0.94884169,\n",
      "       0.94740056, 0.91616486, 0.93451907, 0.93527499, 0.91783354,\n",
      "       0.91388168, 0.92322012, 0.90842182, 0.99546191, 0.99511866,\n",
      "       0.98640372, 0.98544854, 0.98769959, 0.98189906, 0.96546422,\n",
      "       0.97567814, 0.97413416, 0.99993315, 0.99994086, 0.9993495 ,\n",
      "       0.99810377, 0.99878256, 0.99832103, 0.98716221, 0.993288  ,\n",
      "       0.99207312, 0.94834675, 0.9457833 , 0.90999152, 0.93446764,\n",
      "       0.93592677, 0.91836706, 0.91317332, 0.92071452, 0.90917774,\n",
      "       0.99460057, 0.99449644, 0.98572365, 0.98535726, 0.98744761,\n",
      "       0.97844985, 0.96621371, 0.97378963, 0.97199367, 0.99987144,\n",
      "       0.99988558, 0.99904482, 0.99738256, 0.99858843, 0.99721415,\n",
      "       0.9849703 , 0.99291261, 0.99128892, 0.91834777, 0.91185303,\n",
      "       0.81505605, 0.90671072, 0.90745764, 0.81383539, 0.88641529,\n",
      "       0.88942355, 0.81348829, 0.96890957, 0.96504384, 0.90043453,\n",
      "       0.95322423, 0.95848096, 0.90172268, 0.93383385, 0.94050472,\n",
      "       0.89181986, 0.98680354, 0.98576608, 0.95382331, 0.97589155,\n",
      "       0.98078317, 0.94658679, 0.9564581 , 0.96634999, 0.94155375,\n",
      "       0.95623506, 0.95382331, 0.91677423, 0.94148561, 0.94164631,\n",
      "       0.91941866, 0.91927468, 0.92798061, 0.91537166, 0.9964878 ,\n",
      "       0.99657008, 0.98627774, 0.98841051, 0.98873705, 0.98248528,\n",
      "       0.96898671, 0.97812203, 0.97180855, 0.99997557, 0.999964  ,\n",
      "       0.99954362, 0.99872471, 0.99934693, 0.9982709 , 0.98807883,\n",
      "       0.99466614, 0.99298331, 0.95675314, 0.95389273, 0.91507212,\n",
      "       0.94072584, 0.94164631, 0.91929332, 0.92021315, 0.92578227,\n",
      "       0.91401666, 0.99629882, 0.99617798, 0.98583036, 0.98832695,\n",
      "       0.98863034, 0.98255342, 0.96866146, 0.9787391 , 0.97257861,\n",
      "       0.99995886, 0.99996143, 0.99966575, 0.99875685, 0.99928008,\n",
      "       0.99823747, 0.98918443, 0.99487954, 0.99284576, 0.95607436,\n",
      "       0.95254545, 0.91576762, 0.94091803, 0.94065256, 0.92201167,\n",
      "       0.91964878, 0.92616923, 0.91393053, 0.99632196, 0.99634639,\n",
      "       0.98548839, 0.98793485, 0.98963181, 0.98342504, 0.96834392,\n",
      "       0.97709356, 0.96980433, 0.99991001, 0.99991901, 0.99950248,\n",
      "       0.99849073, 0.99904482, 0.99817962, 0.98823824, 0.99480498,\n",
      "       0.99305531, 0.95457537, 0.95170468, 0.89961047, 0.94084025,\n",
      "       0.94066542, 0.92273674, 0.91981591, 0.92632093, 0.91265138,\n",
      "       0.99622426, 0.99550176, 0.98162266, 0.98645643, 0.98858406,\n",
      "       0.9742023 , 0.96786311, 0.97810789, 0.97176548, 0.99985216,\n",
      "       0.99987916, 0.99916052, 0.99854087, 0.99909752, 0.99652765,\n",
      "       0.98744504, 0.99406448, 0.99157689, 0.92918263, 0.91950994,\n",
      "       0.79887383, 0.91750572, 0.91585761, 0.80321523, 0.8981192 ,\n",
      "       0.90039724, 0.79928393, 0.97343609, 0.96922582, 0.88287095,\n",
      "       0.96236984, 0.96168077, 0.89343455, 0.94188671, 0.948663  ,\n",
      "       0.87186255, 0.98860463, 0.9890893 , 0.95502147, 0.98031522,\n",
      "       0.98309207, 0.94660736, 0.96314761, 0.97080323, 0.94036073,\n",
      "       0.95696269, 0.95631862, 0.91499756, 0.94258607, 0.94345898,\n",
      "       0.92632222, 0.92399339, 0.9302998 , 0.9136657 , 0.9971833 ,\n",
      "       0.99697246, 0.98575451, 0.98953282, 0.99059085, 0.98393798,\n",
      "       0.97120304, 0.9796133 , 0.9715283 , 0.99995886, 0.99995886,\n",
      "       0.99911681, 0.99911424, 0.99933793, 0.99845731, 0.98995963,\n",
      "       0.9951238 , 0.99221711, 0.95753092, 0.95461394, 0.91685265,\n",
      "       0.94201527, 0.94342619, 0.92458283, 0.92399339, 0.9298357 ,\n",
      "       0.9136657 , 0.99677963, 0.99703674, 0.98610675, 0.98976937,\n",
      "       0.99013576, 0.9845872 , 0.97170828, 0.97976628, 0.97107577,\n",
      "       0.99994215, 0.99995758, 0.9992955 , 0.99888797, 0.99936235,\n",
      "       0.99806392, 0.98994421, 0.9948744 , 0.99317872, 0.95659759,\n",
      "       0.95411385, 0.91665852, 0.94353354, 0.94260407, 0.92414959,\n",
      "       0.92341167, 0.92986527, 0.9136657 , 0.99664593, 0.99667035,\n",
      "       0.98528141, 0.98932584, 0.99023732, 0.98462063, 0.97170185,\n",
      "       0.98045535, 0.97346181, 0.99993829, 0.99997686, 0.99917723,\n",
      "       0.99881598, 0.99934307, 0.99714859, 0.98941069, 0.99501196,\n",
      "       0.9923251 , 0.95502661, 0.95493726, 0.88324377, 0.9408274 ,\n",
      "       0.94174787, 0.92096393, 0.92310699, 0.92863112, 0.91355128,\n",
      "       0.9962384 , 0.99618183, 0.98242743, 0.98907773, 0.99043787,\n",
      "       0.95729566, 0.9705114 , 0.97856555, 0.96665081, 0.99987273,\n",
      "       0.99985859, 0.99543105, 0.99879798, 0.99899725, 0.9984046 ,\n",
      "       0.98919214, 0.99482812, 0.98623788, 0.93208804, 0.92582856,\n",
      "       0.77030571, 0.9211889 , 0.9182295 , 0.76704934, 0.90216491,\n",
      "       0.90504075, 0.76142108, 0.97667961, 0.97383334, 0.83384799,\n",
      "       0.96490242, 0.96667395, 0.81821536, 0.94585272, 0.95261358,\n",
      "       0.80321395, 0.99019618, 0.99043144, 0.89262464, 0.9831962 ,\n",
      "       0.986495  , 0.86871802, 0.96458103, 0.97349523, 0.87793305]), 'mean_train_score': array([0.95109256, 0.95093432, 0.92593885, 0.93440349, 0.93729616,\n",
      "       0.92590261, 0.91115984, 0.9189864 , 0.91010057, 0.99539592,\n",
      "       0.99641534, 0.98694325, 0.98439876, 0.98879492, 0.98208637,\n",
      "       0.96559062, 0.97411325, 0.96813659, 0.99995151, 0.99998157,\n",
      "       0.99963354, 0.99796754, 0.99902368, 0.99801507, 0.98661025,\n",
      "       0.99300429, 0.99099703, 0.95075475, 0.95061266, 0.92579421,\n",
      "       0.93387185, 0.93743512, 0.92445413, 0.91099678, 0.91902154,\n",
      "       0.90911611, 0.99512839, 0.99610992, 0.98945738, 0.98452374,\n",
      "       0.98857445, 0.98353665, 0.96567706, 0.9737352 , 0.96851811,\n",
      "       0.99993135, 0.99998542, 0.99970184, 0.9980557 , 0.99899216,\n",
      "       0.9985954 , 0.98723781, 0.99276185, 0.99070407, 0.94900732,\n",
      "       0.95018867, 0.92561918, 0.93279682, 0.93702424, 0.9244922 ,\n",
      "       0.91159517, 0.91929761, 0.90712735, 0.99487135, 0.9957684 ,\n",
      "       0.98822654, 0.98412168, 0.98842442, 0.98345306, 0.96496352,\n",
      "       0.97393705, 0.96927257, 0.99990771, 0.99997041, 0.99954952,\n",
      "       0.99793125, 0.99890441, 0.9983361 , 0.9867369 , 0.9928741 ,\n",
      "       0.99040368, 0.94885092, 0.94905731, 0.92124768, 0.9326955 ,\n",
      "       0.93678022, 0.9233854 , 0.9109579 , 0.91888891, 0.90902259,\n",
      "       0.99409847, 0.99505633, 0.98732382, 0.98390571, 0.98817121,\n",
      "       0.98026277, 0.96441448, 0.97301721, 0.96799729, 0.99982314,\n",
      "       0.99990989, 0.99928045, 0.99719649, 0.99870183, 0.99762096,\n",
      "       0.98550933, 0.99229542, 0.98954141, 0.91838862, 0.91295607,\n",
      "       0.82585577, 0.90536469, 0.90458996, 0.82657933, 0.8855535 ,\n",
      "       0.88728597, 0.8070846 , 0.96609393, 0.96685836, 0.91099074,\n",
      "       0.95322039, 0.95762952, 0.91169285, 0.93261094, 0.93808845,\n",
      "       0.89427298, 0.98511838, 0.98655542, 0.96103769, 0.97508194,\n",
      "       0.97999944, 0.95592096, 0.95460201, 0.96410858, 0.94448591,\n",
      "       0.95441298, 0.95315809, 0.9291827 , 0.93785212, 0.94144412,\n",
      "       0.92569231, 0.91639117, 0.92135561, 0.91236886, 0.99626086,\n",
      "       0.99670404, 0.98865571, 0.98703402, 0.98932859, 0.9835703 ,\n",
      "       0.96897711, 0.97615744, 0.96779127, 0.99997897, 0.99998371,\n",
      "       0.99971086, 0.99856515, 0.99924028, 0.9984375 , 0.98814925,\n",
      "       0.99402498, 0.99147459, 0.95464452, 0.95291924, 0.92806171,\n",
      "       0.93781941, 0.9415253 , 0.92469747, 0.91681094, 0.92087838,\n",
      "       0.91191719, 0.99609178, 0.99640498, 0.98849155, 0.98629191,\n",
      "       0.98908901, 0.9852977 , 0.9681875 , 0.97659248, 0.96815619,\n",
      "       0.99996696, 0.99997082, 0.99968972, 0.99849769, 0.99924548,\n",
      "       0.99845385, 0.98882531, 0.99421122, 0.9915898 , 0.95425891,\n",
      "       0.95239756, 0.92812561, 0.93765155, 0.94139892, 0.92689681,\n",
      "       0.91665245, 0.92083127, 0.91265943, 0.995871  , 0.99634901,\n",
      "       0.98829424, 0.98600702, 0.98896327, 0.98444877, 0.96844931,\n",
      "       0.97586787, 0.96828413, 0.99993822, 0.99996098, 0.99963874,\n",
      "       0.99826897, 0.9991589 , 0.9983259 , 0.98778492, 0.99371993,\n",
      "       0.99131691, 0.95335982, 0.95233248, 0.91695732, 0.93635387,\n",
      "       0.94105789, 0.92249646, 0.91719564, 0.92018433, 0.90938461,\n",
      "       0.99551845, 0.99569982, 0.98602898, 0.98533383, 0.98842203,\n",
      "       0.98062462, 0.96772983, 0.97532422, 0.96803805, 0.99988028,\n",
      "       0.99993352, 0.99927478, 0.9980632 , 0.99906695, 0.99766787,\n",
      "       0.98781687, 0.99332921, 0.99046376, 0.92788996, 0.92025993,\n",
      "       0.81206713, 0.91349983, 0.91166626, 0.81282341, 0.89564432,\n",
      "       0.89459118, 0.79652129, 0.97202748, 0.96985482, 0.89715912,\n",
      "       0.96030966, 0.95944474, 0.90303097, 0.94037578, 0.94419233,\n",
      "       0.8768328 , 0.98815272, 0.98878286, 0.95421676, 0.97954324,\n",
      "       0.98288951, 0.94836734, 0.96145755, 0.96834136, 0.93563568,\n",
      "       0.95759932, 0.95640966, 0.92746562, 0.93979542, 0.94343756,\n",
      "       0.9281232 , 0.91946904, 0.92526058, 0.91094163, 0.99712277,\n",
      "       0.99719895, 0.98829876, 0.98800127, 0.99030988, 0.98550968,\n",
      "       0.96999291, 0.97750923, 0.96943346, 0.99998242, 0.99998285,\n",
      "       0.99947625, 0.99880022, 0.99940736, 0.99867401, 0.98951364,\n",
      "       0.99468305, 0.99183466, 0.95728535, 0.95573406, 0.92792335,\n",
      "       0.9397048 , 0.94257515, 0.9280893 , 0.91941407, 0.9249766 ,\n",
      "       0.91085359, 0.99687611, 0.99700434, 0.98919615, 0.98810588,\n",
      "       0.98996019, 0.98528928, 0.97034343, 0.97753274, 0.96936465,\n",
      "       0.99996525, 0.99998285, 0.9995371 , 0.99862172, 0.9994125 ,\n",
      "       0.99835434, 0.98930921, 0.99431043, 0.99251899, 0.95667102,\n",
      "       0.95536722, 0.92737546, 0.93953099, 0.94291357, 0.92767602,\n",
      "       0.91951351, 0.92556177, 0.91152489, 0.99657513, 0.99699946,\n",
      "       0.98882225, 0.98803793, 0.98982439, 0.98531889, 0.97068789,\n",
      "       0.97800724, 0.97053151, 0.9999601 , 0.99998885, 0.99932846,\n",
      "       0.9985436 , 0.99938417, 0.99772366, 0.98907725, 0.99434297,\n",
      "       0.99170573, 0.95574878, 0.95471397, 0.90746038, 0.93959231,\n",
      "       0.94222187, 0.92235843, 0.91975769, 0.92500801, 0.9099397 ,\n",
      "       0.99587493, 0.99658665, 0.98723698, 0.98780919, 0.98958951,\n",
      "       0.9740708 , 0.96936358, 0.97734252, 0.96850213, 0.99991291,\n",
      "       0.99993611, 0.99686597, 0.99849938, 0.99907734, 0.99670093,\n",
      "       0.98908086, 0.99429114, 0.98917501, 0.9327832 , 0.92522273,\n",
      "       0.77957564, 0.91841267, 0.91605609, 0.77708486, 0.90042052,\n",
      "       0.90147813, 0.77134994, 0.97608594, 0.97518915, 0.85068916,\n",
      "       0.96437167, 0.96586885, 0.85374401, 0.94511316, 0.95024528,\n",
      "       0.81191566, 0.98969943, 0.99066091, 0.90934995, 0.98252092,\n",
      "       0.98543773, 0.89267813, 0.96536577, 0.9724827 , 0.87361211]), 'std_train_score': array([1.95213445e-03, 1.76834511e-03, 7.50891165e-03, 3.08858310e-03,\n",
      "       2.72250463e-04, 3.29958807e-03, 3.49379747e-03, 1.92368159e-03,\n",
      "       2.04320532e-03, 3.39117794e-04, 6.19961925e-04, 2.66525875e-03,\n",
      "       2.23950962e-03, 8.50534785e-04, 1.49951614e-03, 1.60217846e-03,\n",
      "       2.01935277e-03, 4.30312149e-03, 7.12115385e-06, 2.15683421e-05,\n",
      "       1.52540297e-04, 3.27292857e-04, 1.53562716e-04, 3.47179278e-04,\n",
      "       6.58158366e-04, 9.75810365e-04, 2.02144114e-03, 2.57571196e-03,\n",
      "       1.94226562e-03, 8.94442868e-03, 2.82419883e-03, 6.66137029e-04,\n",
      "       4.24195175e-03, 3.18182646e-03, 2.48713413e-03, 1.78212297e-03,\n",
      "       6.30096612e-04, 7.69648826e-04, 1.30158996e-03, 1.83834805e-03,\n",
      "       6.46656597e-04, 1.17479963e-03, 1.22457711e-03, 2.02780058e-03,\n",
      "       3.45724028e-03, 2.94093796e-05, 9.68817938e-06, 1.15247935e-04,\n",
      "       2.01815691e-04, 1.59858483e-04, 2.66982585e-04, 8.37615960e-04,\n",
      "       8.90482450e-04, 1.52266581e-03, 2.87239394e-03, 2.06673977e-03,\n",
      "       7.24893239e-03, 3.70946861e-03, 1.31013764e-03, 4.80274377e-03,\n",
      "       3.09182399e-03, 3.47317151e-03, 2.19994464e-03, 6.90326455e-04,\n",
      "       4.90766063e-04, 1.55157378e-03, 1.61545025e-03, 5.69737120e-04,\n",
      "       1.27571750e-03, 1.70262038e-03, 1.54292703e-03, 4.58931458e-03,\n",
      "       2.42807278e-05, 2.10570207e-05, 1.64048145e-04, 1.45572710e-04,\n",
      "       1.65221133e-04, 1.39094357e-05, 1.20435747e-03, 9.26933584e-04,\n",
      "       1.82946979e-03, 1.87632789e-03, 2.52410581e-03, 8.20074277e-03,\n",
      "       2.55872679e-03, 6.45404628e-04, 4.94602981e-03, 3.37174195e-03,\n",
      "       2.37818094e-03, 6.64029739e-04, 5.93448034e-04, 4.07439321e-04,\n",
      "       1.41207620e-03, 1.51330472e-03, 6.05352558e-04, 1.34080205e-03,\n",
      "       2.09324571e-03, 1.12881438e-03, 2.93911218e-03, 3.85519159e-05,\n",
      "       1.98830275e-05, 3.12238969e-04, 2.41948865e-04, 1.42960484e-04,\n",
      "       3.65613323e-04, 9.33519000e-04, 8.23681916e-04, 2.34195482e-03,\n",
      "       1.24649353e-03, 8.48067898e-04, 7.65860387e-03, 3.57503088e-03,\n",
      "       2.09454784e-03, 9.30854088e-03, 1.96531031e-03, 1.60302151e-03,\n",
      "       6.01398416e-03, 2.24110775e-03, 1.48571455e-03, 7.73463974e-03,\n",
      "       1.19198118e-03, 6.17396823e-04, 7.40138410e-03, 2.94410334e-03,\n",
      "       1.83452399e-03, 5.04053857e-03, 1.76722705e-03, 8.67306414e-04,\n",
      "       5.23588859e-03, 2.24462078e-03, 1.61066976e-03, 6.61126401e-03,\n",
      "       2.58197124e-03, 2.28307277e-03, 2.99965929e-03, 1.79648019e-03,\n",
      "       5.21483268e-04, 9.03229019e-03, 3.38631083e-03, 2.25330733e-03,\n",
      "       4.45731313e-03, 3.42643505e-03, 5.76788999e-03, 2.60927152e-03,\n",
      "       1.72440114e-04, 2.07049405e-04, 2.24402707e-03, 1.68723804e-03,\n",
      "       9.40325643e-04, 7.78237127e-04, 2.02679496e-03, 1.70370219e-03,\n",
      "       3.46259294e-03, 1.38862912e-05, 1.40897440e-05, 1.21746349e-04,\n",
      "       2.83776190e-04, 1.78706472e-04, 1.19156869e-04, 9.14540300e-04,\n",
      "       7.04857183e-04, 1.66171383e-03, 1.84412393e-03, 8.09008414e-04,\n",
      "       1.01445258e-02, 2.80602580e-03, 1.78213225e-03, 4.11689104e-03,\n",
      "       3.75622135e-03, 4.56765274e-03, 2.12220917e-03, 1.86289853e-04,\n",
      "       2.47443413e-04, 2.20637375e-03, 1.96829819e-03, 8.89222893e-04,\n",
      "       2.01771799e-03, 2.58514683e-03, 1.80618811e-03, 3.17982627e-03,\n",
      "       2.12992581e-05, 6.96554437e-06, 6.18114152e-05, 2.18147415e-04,\n",
      "       1.00852154e-04, 2.79209026e-04, 9.23531705e-04, 8.34705569e-04,\n",
      "       1.01013570e-03, 1.66241089e-03, 4.56628403e-04, 9.39701513e-03,\n",
      "       3.58330767e-03, 2.69958526e-03, 3.49860625e-03, 3.54832397e-03,\n",
      "       4.38208287e-03, 3.86495302e-03, 3.19408734e-04, 4.85526849e-04,\n",
      "       2.12184517e-03, 1.57983405e-03, 9.23427364e-04, 1.33570952e-03,\n",
      "       2.54864048e-03, 1.96426445e-03, 2.35108795e-03, 2.67916456e-05,\n",
      "       2.96760723e-05, 2.03613575e-04, 1.64261854e-04, 1.48493654e-04,\n",
      "       2.58806668e-04, 8.02212375e-04, 8.59869141e-04, 1.46438755e-03,\n",
      "       1.59667410e-03, 1.00591727e-03, 1.26428092e-02, 4.27546610e-03,\n",
      "       1.84161106e-03, 5.50960113e-03, 3.21120212e-03, 5.02727227e-03,\n",
      "       2.45068015e-03, 5.68113372e-04, 4.56010886e-04, 3.29330640e-03,\n",
      "       1.42948252e-03, 7.72857400e-04, 4.55574986e-03, 2.31906646e-03,\n",
      "       2.21234070e-03, 2.93067933e-03, 4.09259931e-05, 3.92422001e-05,\n",
      "       3.00892009e-04, 3.94365435e-04, 1.19796855e-04, 9.60100894e-04,\n",
      "       7.66133981e-04, 7.24426041e-04, 9.08896421e-04, 2.32885940e-03,\n",
      "       2.55634769e-03, 1.05641348e-02, 4.06324449e-03, 3.74107142e-03,\n",
      "       1.11301230e-02, 4.66709337e-03, 5.03434404e-03, 2.04726949e-03,\n",
      "       1.84040168e-03, 2.10585002e-03, 1.43415594e-02, 2.22373556e-03,\n",
      "       2.00342982e-03, 6.82390730e-03, 3.37447499e-03, 3.70354985e-03,\n",
      "       6.18705076e-03, 4.40358660e-04, 1.17582003e-03, 9.08753643e-04,\n",
      "       7.02961670e-04, 1.19428576e-03, 4.41158794e-03, 1.76104799e-03,\n",
      "       2.72399069e-03, 4.02681824e-03, 1.22899540e-03, 2.57514151e-04,\n",
      "       9.85286673e-03, 2.24544197e-03, 2.48526261e-03, 1.83099934e-03,\n",
      "       3.31747520e-03, 3.64147149e-03, 2.14960168e-03, 5.82100619e-05,\n",
      "       3.51111577e-04, 3.25814064e-03, 1.23159967e-03, 7.37911654e-04,\n",
      "       1.36725113e-03, 1.15049993e-03, 1.87300148e-03, 1.61578438e-03,\n",
      "       1.73191812e-05, 1.74774533e-05, 3.13224172e-04, 2.76113974e-04,\n",
      "       1.14195103e-04, 2.15481538e-04, 3.69762271e-04, 5.84043314e-04,\n",
      "       7.87978681e-04, 1.06836418e-03, 7.92852789e-04, 8.56243304e-03,\n",
      "       2.41809578e-03, 2.78996894e-03, 2.63649488e-03, 3.37322438e-03,\n",
      "       3.56203024e-03, 2.25431483e-03, 7.10931171e-05, 4.07264294e-04,\n",
      "       2.57455760e-03, 1.23524867e-03, 7.62395151e-04, 5.33428985e-04,\n",
      "       9.76585383e-04, 1.98726827e-03, 1.32700910e-03, 2.13068404e-05,\n",
      "       1.79425243e-05, 2.53872253e-04, 2.22663543e-04, 5.70652455e-05,\n",
      "       4.28519784e-04, 5.01119568e-04, 4.91137542e-04, 6.12319640e-04,\n",
      "       1.13740491e-03, 9.02903476e-04, 8.42387243e-03, 3.38499539e-03,\n",
      "       3.14222811e-03, 3.27902699e-03, 3.17775705e-03, 3.42897455e-03,\n",
      "       2.54531847e-03, 1.43173142e-04, 3.50399784e-04, 2.75518221e-03,\n",
      "       9.34802023e-04, 8.89362940e-04, 8.43457545e-04, 7.84004734e-04,\n",
      "       1.97386370e-03, 2.08462624e-03, 1.73239442e-05, 8.73605436e-06,\n",
      "       1.22858144e-04, 1.92646178e-04, 7.62427462e-05, 6.85450859e-04,\n",
      "       2.41920673e-04, 6.68509703e-04, 1.00471808e-03, 1.40191182e-03,\n",
      "       1.92698150e-04, 1.77286709e-02, 1.45107776e-03, 2.83465723e-03,\n",
      "       7.41107606e-03, 2.64173010e-03, 2.61285924e-03, 4.31241885e-03,\n",
      "       3.22708954e-04, 4.30001397e-04, 3.62180917e-03, 1.02859454e-03,\n",
      "       1.09369119e-03, 1.19149170e-02, 8.11816746e-04, 1.40540268e-03,\n",
      "       1.45885046e-03, 3.06781692e-05, 5.48211640e-05, 1.07496890e-03,\n",
      "       2.11554442e-04, 7.11861927e-05, 1.30971683e-03, 2.29084827e-04,\n",
      "       6.94727893e-04, 2.20338600e-03, 1.93915010e-03, 1.32868295e-03,\n",
      "       8.74514399e-03, 3.35747953e-03, 2.61665184e-03, 1.84785665e-02,\n",
      "       1.47816094e-03, 2.84663532e-03, 1.02478825e-02, 1.21325996e-03,\n",
      "       1.98165565e-03, 1.29933973e-02, 7.25351424e-04, 2.36767387e-03,\n",
      "       2.51993780e-02, 1.28192835e-03, 2.72928910e-03, 6.17027449e-03,\n",
      "       3.62198571e-04, 1.01357565e-03, 1.25406776e-02, 5.90677765e-04,\n",
      "       1.13836990e-03, 2.12981292e-02, 5.67052732e-04, 2.10148235e-03,\n",
      "       8.80174691e-03])}\n",
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.6, gamma=5, learning_rate=0.02, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=5, missing=None, n_estimators=600,\n",
      "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)\n",
      "\n",
      " Best score:\n",
      "0.18774458443024167\n",
      "\n",
      " Best parameters:\n",
      "{'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-95dfe7d2f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xgb-grid-search-results-01.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mresults_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission-grid-search-xgb-porto-01.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=xgb, param_grid=params, scoring='roc_auc', n_jobs=-1, cv=skf.split(X,y), verbose=3 )\n",
    "grid.fit(X, y)\n",
    "print('\\n All results:')\n",
    "print(grid.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(grid.best_estimator_)\n",
    "print('\\n Best score:')\n",
    "print(grid.best_score_ * 2 - 1)\n",
    "print('\\n Best parameters:')\n",
    "print(grid.best_params_)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.to_csv('xgb-grid-search-results-01.csv', index=False)\n",
    "\n",
    "y_test = grid.best_estimator_.predict_proba(X_test)\n",
    "results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n",
    "results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.6, gamma=5, learning_rate=0.02, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=5, missing=None, n_estimators=600,\n",
       "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1.0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = grid.best_estimator_.predict_proba(X_test)\n",
    "#results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n",
    "#results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.6931\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.67      0.68      1175\n",
      "        1.0       0.69      0.71      0.70      1204\n",
      "\n",
      "avg / total       0.69      0.69      0.69      2379\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[791 384]\n",
      " [346 858]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: \t 0.5679\n",
      "Accuracy SD: \t\t 0.0259\n",
      "Test Result:\n",
      "\n",
      "accuracy score: 0.6717\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.63      0.66       135\n",
      "        1.0       0.65      0.72      0.68       130\n",
      "\n",
      "avg / total       0.67      0.67      0.67       265\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 50]\n",
      " [37 93]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Nikhil\\AppData\\Local\\conda\\conda\\envs\\MachineLearningTutorials\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print_score(grid.best_estimator_, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(grid.best_estimator_, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
